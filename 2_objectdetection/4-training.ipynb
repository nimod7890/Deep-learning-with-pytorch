{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ssd_model\n",
    "지금까지 구현한 모든 class를 ssd_model.py에 저장 후 load해 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ssd_model import makeDatapathList,VOCDataset,dataTransform,anno_xml2list,od_collate_fn\n",
    "\n",
    "root_path=\"./data/VOCdevkit/VOC2012/\"\n",
    "\n",
    "datapath_list=makeDatapathList(root_path)\n",
    "train_img_list,train_anno_list=datapath_list('train')\n",
    "val_img_list,val_anno_list=datapath_list('val')\n",
    "\n",
    "voc_classes=['aeroplane','bicycle','bird','boat','bottle','bus','car','cat','chair','cow','diningtable','dog','horse','motorbike','person','pottedplant','sheep','sofa','train','tvmonitor']\n",
    "color_mean=(104,117,123)\n",
    "input_size=300\n",
    "\n",
    "train_dataset=VOCDataset(train_img_list,train_anno_list,phase=\"train\",transform=dataTransform(input_size,color_mean),transform_anno=anno_xml2list(voc_classes))\n",
    "val_dataset=VOCDataset(val_img_list,val_anno_list,phase=\"val\",transform=dataTransform(input_size,color_mean),transform_anno=anno_xml2list(voc_classes))\n",
    "\n",
    "batch_size=32\n",
    "train_dataloader=data.DataLoader(train_dataset,batch_size=batch_size,shuffle=True,collate_fn=od_collate_fn)\n",
    "val_dataloader=data.DataLoader(val_dataset,batch_size=batch_size,shuffle=False,collate_fn=od_collate_fn)\n",
    "dataloaders_dict={\"train\":train_dataloader,\"val\":val_dataloader}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG 외 모듈의 초기 값으로 He를 사용\n",
    "He: ReLU를 activation function인 경우 사용\n",
    "- Kaiming He가 처음 제안해서 함수 명이 kaiming_normal_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): Conv2d(512, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): Conv2d(1024, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (2): Conv2d(512, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): Conv2d(256, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (4): Conv2d(256, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (5): Conv2d(256, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ssd_model import SSD\n",
    "\n",
    "ssd_cfg={\n",
    "    'num_classes':21, #include background class\n",
    "    'input_size':300, \n",
    "    'bbox_aspect_num':[4,6,6,6,4,4], #DBox 화면비 \n",
    "    'feature_maps':[38,19,10,5,3,1], #각 source 별 화상 크기\n",
    "    'steps':[8,16,32,64,100,300], \n",
    "    'min_sizes':[30,60,111,162,213,264], #DBox 최소 크기\n",
    "    'max_sizes':[60,111,162,213,264,315],\n",
    "    'aspect_ratios':[[2],[2,3],[2,3],[2,3],[2],[2]]\n",
    "}\n",
    "\n",
    "net=SSD(phase=\"train\",cfg=ssd_cfg)\n",
    "\n",
    "#load weight of vgg\n",
    "vgg_weights=torch.load('./weights/vgg16_reducedfc.pth')\n",
    "net.vgg.load_state_dict(vgg_weights)\n",
    "\n",
    "\n",
    "#vgg 제외한 네트워크 He 초기화\n",
    "def weights_init(m):\n",
    "    if isinstance(m,nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias,0.0)\n",
    "\n",
    "net.extras.apply(weights_init)\n",
    "net.loc.apply(weights_init)\n",
    "net.conf.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ssd_model import MultiBoxLoss\n",
    "\n",
    "criterion=MultiBoxLoss(jaccard_thresh=0.5, neg_pos=3,device=device)\n",
    "\n",
    "optimizer=optim.SGD(net.parameters(),lr=1e-3,momentum=0.9,weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(net,dataloaders_dict,criterion,optimizer,num_epochs):\n",
    "    device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "    net.to(device)\n",
    "\n",
    "    torch.backends.cudnn.benchmark=True\n",
    "\n",
    "    iteration=1\n",
    "    epoch_train_loss=0.0\n",
    "    epoch_val_loss=0.0\n",
    "    logs=[]\n",
    "\n",
    "    for epoch in range(num_epochs+1):\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        t_epoch_start=time.time()\n",
    "        t_iter_start=time.time()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        for phase in ['train','val']:\n",
    "            if phase==\"train\":\n",
    "                net.train()\n",
    "                print(\"[train]\")\n",
    "            else:\n",
    "                if (epoch+1)%10==0:\n",
    "                    net.eval()\n",
    "                    print(\"[val]\")\n",
    "                else:\n",
    "                    continue\n",
    "            \n",
    "            for images,targets in dataloaders_dict[phase]:\n",
    "                images=images.to(device)\n",
    "                targets=[target.to(device) for target in targets]\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "                with torch.set_grad_enabled(phase=='train'):\n",
    "                    outputs=net(images)\n",
    "                    loss_l,loss_c=criterion(outputs,targets)\n",
    "                    loss=loss_l+loss_c\n",
    "                    if phase==\"train\":\n",
    "                        loss.backward()\n",
    "                        \n",
    "                        nn.utils.clip_grad_value_(net.parameters(),clip_value=2.0)\n",
    "\n",
    "                        optimizer.step()\n",
    "\n",
    "                        if iteration%10==0:\n",
    "                            t_iter_finish=time.time()\n",
    "                            print(\"[%s] loss:%.4f || sec:%.4f\"%(iteration,loss.item(),t_iter_finish-t_iter_start))\n",
    "                            t_iter_start=time.time()\n",
    "                        \n",
    "                        epoch_train_loss+=loss.item()\n",
    "                        iteration+=1\n",
    "                    else:\n",
    "                        epoch_val_loss+=loss.item()\n",
    "        t_epoch_finish=time.time()\n",
    "        print(\"[%s] train_loss: %.4f || val_loss: %.4f || sec:%.4f\"%(epoch+1,epoch_train_loss,epoch_val_loss,t_epoch_finish-t_epoch_start))\n",
    "        t_epoch_start=time.time()\n",
    "\n",
    "        log_epoch={'epoch':epoch+1,'train_loss':epoch_train_loss,'val_loss':epoch_val_loss}\n",
    "        logs.append(log_epoch)\n",
    "        df=pd.DataFrame(logs)\n",
    "        df.to_csv(\"log_output.csv\")\n",
    "\n",
    "        epoch_train_loss=0.0\n",
    "        epoch_val_loss=0.0\n",
    "        if (epoch+1)%10==0:\n",
    "            torch.save(net.state_dict(),f'weights/ssd300_{str(epoch+1)}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Epoch 1/50\n",
      "[train]\n",
      "[10] loss:15.6766 || sec:9.2637\n",
      "[20] loss:12.4908 || sec:6.5286\n",
      "[30] loss:10.8597 || sec:6.3939\n",
      "[40] loss:9.8142 || sec:6.3378\n",
      "[50] loss:9.4655 || sec:6.4636\n",
      "[60] loss:8.8303 || sec:6.6334\n",
      "[70] loss:9.4185 || sec:6.5873\n",
      "[80] loss:9.0911 || sec:6.5910\n",
      "[90] loss:10.2271 || sec:6.6398\n",
      "[100] loss:10.3789 || sec:6.3639\n",
      "[110] loss:11.5522 || sec:6.4344\n",
      "[120] loss:8.9005 || sec:6.4239\n",
      "[130] loss:9.0235 || sec:6.6121\n",
      "[140] loss:9.3548 || sec:6.4601\n",
      "[150] loss:9.0980 || sec:6.6050\n",
      "[160] loss:8.8569 || sec:6.6848\n",
      "[170] loss:11.2518 || sec:6.2628\n",
      "[1] train_loss: 1888.9946 || val_loss: 0.0000 || sec:122.9623\n",
      "Epoch 2/50\n",
      "[train]\n",
      "[180] loss:10.5356 || sec:0.5834\n",
      "[190] loss:9.0827 || sec:6.5933\n",
      "[200] loss:9.7388 || sec:6.6798\n",
      "[210] loss:9.2381 || sec:6.6137\n",
      "[220] loss:9.2128 || sec:6.5304\n",
      "[230] loss:10.8955 || sec:6.2691\n",
      "[240] loss:8.8872 || sec:6.1577\n",
      "[250] loss:9.2147 || sec:6.5017\n",
      "[260] loss:8.7539 || sec:6.2838\n",
      "[270] loss:8.3689 || sec:6.6832\n",
      "[280] loss:10.6971 || sec:6.6137\n",
      "[290] loss:9.6367 || sec:6.3583\n",
      "[300] loss:8.9407 || sec:6.2236\n",
      "[310] loss:11.8779 || sec:6.4600\n",
      "[320] loss:7.8944 || sec:6.4371\n",
      "[330] loss:11.4134 || sec:6.7462\n",
      "[340] loss:9.3739 || sec:6.7902\n",
      "[350] loss:9.7319 || sec:6.6116\n",
      "[2] train_loss: 1732.9511 || val_loss: 0.0000 || sec:119.0615\n",
      "Epoch 3/50\n",
      "[train]\n",
      "[360] loss:8.2982 || sec:1.1190\n",
      "[370] loss:9.0092 || sec:6.3197\n",
      "[380] loss:9.8833 || sec:6.3650\n",
      "[390] loss:12.0387 || sec:6.2284\n",
      "[400] loss:7.5948 || sec:6.4911\n",
      "[410] loss:9.5396 || sec:6.4556\n",
      "[420] loss:9.8103 || sec:6.4623\n",
      "[430] loss:9.4091 || sec:6.1764\n",
      "[440] loss:10.5256 || sec:6.6257\n",
      "[450] loss:10.5760 || sec:6.4106\n",
      "[460] loss:9.5219 || sec:6.3478\n",
      "[470] loss:8.9018 || sec:6.1702\n",
      "[480] loss:8.6314 || sec:6.3459\n",
      "[490] loss:13.4505 || sec:6.2733\n",
      "[500] loss:9.9531 || sec:6.6228\n",
      "[510] loss:11.2834 || sec:6.5550\n",
      "[520] loss:9.7448 || sec:6.5616\n",
      "[530] loss:8.3744 || sec:6.1399\n",
      "[3] train_loss: 1759.9556 || val_loss: 0.0000 || sec:116.8897\n",
      "Epoch 4/50\n",
      "[train]\n",
      "[540] loss:9.7213 || sec:1.7680\n",
      "[550] loss:9.0340 || sec:6.3106\n",
      "[560] loss:8.9422 || sec:6.7971\n",
      "[570] loss:8.3988 || sec:7.0740\n",
      "[580] loss:10.9450 || sec:6.9593\n",
      "[590] loss:8.1576 || sec:6.6621\n",
      "[600] loss:9.7640 || sec:6.7064\n",
      "[610] loss:11.2866 || sec:7.2069\n",
      "[620] loss:9.2515 || sec:6.9575\n",
      "[630] loss:9.6118 || sec:6.9463\n",
      "[640] loss:9.3284 || sec:6.9102\n",
      "[650] loss:7.7132 || sec:7.0004\n",
      "[660] loss:9.3555 || sec:6.9589\n",
      "[670] loss:10.2978 || sec:6.8485\n",
      "[680] loss:11.9391 || sec:7.4200\n",
      "[690] loss:10.4924 || sec:6.6073\n",
      "[700] loss:9.8247 || sec:6.6449\n",
      "[710] loss:10.7263 || sec:6.5250\n",
      "[4] train_loss: 1770.0775 || val_loss: 0.0000 || sec:124.7816\n",
      "Epoch 5/50\n",
      "[train]\n",
      "[720] loss:10.3776 || sec:2.4261\n",
      "[730] loss:8.6944 || sec:6.4350\n",
      "[740] loss:11.0335 || sec:6.7859\n",
      "[750] loss:10.4353 || sec:6.9698\n",
      "[760] loss:9.9557 || sec:7.5393\n",
      "[770] loss:8.7309 || sec:6.8649\n",
      "[780] loss:9.6923 || sec:6.7604\n",
      "[790] loss:8.9474 || sec:6.8457\n",
      "[800] loss:10.1930 || sec:6.5848\n",
      "[810] loss:10.6915 || sec:6.9965\n",
      "[820] loss:9.1289 || sec:7.3504\n",
      "[830] loss:9.2622 || sec:6.6267\n",
      "[840] loss:9.8376 || sec:7.0665\n",
      "[850] loss:10.4137 || sec:6.4038\n",
      "[860] loss:11.2363 || sec:7.1687\n",
      "[870] loss:10.3701 || sec:6.9677\n",
      "[880] loss:9.2949 || sec:6.9752\n",
      "[890] loss:8.8634 || sec:6.8526\n",
      "[5] train_loss: 1743.7571 || val_loss: 0.0000 || sec:125.3776\n",
      "Epoch 6/50\n",
      "[train]\n",
      "[900] loss:9.0688 || sec:3.3193\n",
      "[910] loss:10.7634 || sec:6.3797\n",
      "[920] loss:8.6749 || sec:6.4384\n",
      "[930] loss:8.1067 || sec:6.1968\n",
      "[940] loss:8.8939 || sec:6.7879\n",
      "[950] loss:9.9082 || sec:6.5684\n",
      "[960] loss:10.8431 || sec:6.7347\n",
      "[970] loss:8.5181 || sec:6.4391\n",
      "[980] loss:9.1216 || sec:6.4015\n",
      "[990] loss:9.3448 || sec:6.3011\n",
      "[1000] loss:10.9103 || sec:6.5498\n",
      "[1010] loss:10.4394 || sec:6.5253\n",
      "[1020] loss:11.2879 || sec:6.2676\n",
      "[1030] loss:9.4518 || sec:6.4558\n",
      "[1040] loss:8.5986 || sec:6.7807\n",
      "[1050] loss:8.0369 || sec:6.6295\n",
      "[1060] loss:8.8165 || sec:6.8239\n",
      "[1070] loss:8.6197 || sec:6.4379\n",
      "[6] train_loss: 1723.4274 || val_loss: 0.0000 || sec:119.3571\n",
      "Epoch 7/50\n",
      "[train]\n",
      "[1080] loss:9.4903 || sec:3.7371\n",
      "[1090] loss:9.7896 || sec:6.3851\n",
      "[1100] loss:9.5237 || sec:6.5409\n",
      "[1110] loss:9.1244 || sec:6.5367\n",
      "[1120] loss:10.6435 || sec:6.3748\n",
      "[1130] loss:10.1186 || sec:6.5417\n",
      "[1140] loss:11.6988 || sec:6.6018\n",
      "[1150] loss:11.2415 || sec:6.2951\n",
      "[1160] loss:9.2050 || sec:6.4594\n",
      "[1170] loss:9.2697 || sec:6.3670\n",
      "[1180] loss:13.4469 || sec:6.4925\n",
      "[1190] loss:8.8383 || sec:6.3885\n",
      "[1200] loss:9.6729 || sec:6.5479\n",
      "[1210] loss:8.6559 || sec:6.3656\n",
      "[1220] loss:8.5243 || sec:6.4339\n",
      "[1230] loss:9.0699 || sec:6.2996\n",
      "[1240] loss:10.3964 || sec:6.7437\n",
      "[1250] loss:9.7816 || sec:6.6405\n",
      "[7] train_loss: 1727.7427 || val_loss: 0.0000 || sec:118.3972\n",
      "Epoch 8/50\n",
      "[train]\n",
      "[1260] loss:11.1792 || sec:4.4707\n",
      "[1270] loss:8.6334 || sec:6.2801\n",
      "[1280] loss:10.0152 || sec:6.3796\n",
      "[1290] loss:11.6424 || sec:6.2299\n",
      "[1300] loss:9.2957 || sec:6.3384\n",
      "[1310] loss:10.6296 || sec:6.5017\n",
      "[1320] loss:7.9429 || sec:6.4566\n",
      "[1330] loss:10.9403 || sec:6.4137\n",
      "[1340] loss:10.1571 || sec:6.5482\n",
      "[1350] loss:10.3995 || sec:6.4600\n",
      "[1360] loss:8.3125 || sec:6.6786\n",
      "[1370] loss:8.9792 || sec:6.4570\n",
      "[1380] loss:10.3696 || sec:6.3015\n",
      "[1390] loss:10.3778 || sec:6.1455\n",
      "[1400] loss:11.0762 || sec:6.2302\n",
      "[1410] loss:9.5046 || sec:6.4284\n",
      "[1420] loss:9.2132 || sec:6.3900\n",
      "[1430] loss:8.8089 || sec:6.4894\n",
      "[8] train_loss: 1745.8715 || val_loss: 0.0000 || sec:117.0712\n",
      "Epoch 9/50\n",
      "[train]\n",
      "[1440] loss:10.3299 || sec:5.0258\n",
      "[1450] loss:11.1114 || sec:6.4102\n",
      "[1460] loss:9.7171 || sec:6.4850\n",
      "[1470] loss:12.5804 || sec:6.5144\n",
      "[1480] loss:9.0997 || sec:6.5993\n",
      "[1490] loss:9.9250 || sec:6.7406\n",
      "[1500] loss:9.4205 || sec:6.2743\n",
      "[1510] loss:9.1769 || sec:6.4909\n",
      "[1520] loss:10.0373 || sec:6.3767\n",
      "[1530] loss:9.9690 || sec:6.3613\n",
      "[1540] loss:10.0738 || sec:6.4349\n",
      "[1550] loss:11.0780 || sec:6.2174\n",
      "[1560] loss:7.8184 || sec:6.5786\n",
      "[1570] loss:8.2786 || sec:6.3053\n",
      "[1580] loss:11.3784 || sec:6.5459\n",
      "[1590] loss:8.6949 || sec:6.3426\n",
      "[1600] loss:11.5361 || sec:6.5125\n",
      "[1610] loss:11.7867 || sec:6.4667\n",
      "[9] train_loss: 1727.5674 || val_loss: 0.0000 || sec:117.8462\n",
      "Epoch 10/50\n",
      "[train]\n",
      "[1620] loss:8.5638 || sec:5.7815\n",
      "[1630] loss:12.0892 || sec:6.3312\n",
      "[1640] loss:10.2568 || sec:6.3970\n",
      "[1650] loss:8.6941 || sec:6.3793\n",
      "[1660] loss:10.9272 || sec:6.3858\n",
      "[1670] loss:9.5100 || sec:6.3478\n",
      "[1680] loss:9.0311 || sec:6.3423\n",
      "[1690] loss:8.0780 || sec:6.2409\n",
      "[1700] loss:13.1162 || sec:6.3314\n",
      "[1710] loss:12.3232 || sec:6.5115\n",
      "[1720] loss:10.5525 || sec:6.5804\n",
      "[1730] loss:9.3243 || sec:6.4007\n",
      "[1740] loss:10.6427 || sec:6.1814\n",
      "[1750] loss:9.1429 || sec:6.2988\n",
      "[1760] loss:8.5694 || sec:6.1993\n",
      "[1770] loss:7.9889 || sec:6.3733\n",
      "[1780] loss:8.5393 || sec:6.4206\n",
      "[1790] loss:9.8856 || sec:6.4288\n",
      "[val]\n",
      "[10] train_loss: 1694.7205 || val_loss: 1680.0766 || sec:165.5704\n",
      "Epoch 11/50\n",
      "[train]\n",
      "[1800] loss:10.3758 || sec:6.4770\n",
      "[1810] loss:9.7744 || sec:6.2962\n",
      "[1820] loss:8.9056 || sec:6.4420\n",
      "[1830] loss:10.7823 || sec:6.3045\n",
      "[1840] loss:10.0066 || sec:6.3215\n",
      "[1850] loss:8.5612 || sec:6.2723\n",
      "[1860] loss:8.9837 || sec:6.0628\n",
      "[1870] loss:7.5766 || sec:6.7663\n",
      "[1880] loss:8.6541 || sec:6.3020\n",
      "[1890] loss:9.7910 || sec:6.4823\n",
      "[1900] loss:8.7319 || sec:6.1312\n",
      "[1910] loss:11.3498 || sec:6.2976\n",
      "[1920] loss:9.1111 || sec:6.3618\n",
      "[1930] loss:8.5764 || sec:6.5723\n",
      "[1940] loss:10.3842 || sec:6.4492\n",
      "[1950] loss:9.1059 || sec:6.3307\n",
      "[1960] loss:9.1178 || sec:6.1840\n",
      "[11] train_loss: 1702.5034 || val_loss: 0.0000 || sec:116.4858\n",
      "Epoch 12/50\n",
      "[train]\n",
      "[1970] loss:9.8307 || sec:0.4959\n",
      "[1980] loss:8.2222 || sec:6.4681\n",
      "[1990] loss:10.5471 || sec:6.2940\n",
      "[2000] loss:8.2901 || sec:6.3885\n",
      "[2010] loss:10.3691 || sec:6.5083\n",
      "[2020] loss:8.4732 || sec:6.4076\n",
      "[2030] loss:9.0995 || sec:6.0551\n",
      "[2040] loss:8.4147 || sec:6.2122\n",
      "[2050] loss:11.2885 || sec:6.0369\n",
      "[2060] loss:9.7334 || sec:6.5561\n",
      "[2070] loss:12.7736 || sec:6.2429\n",
      "[2080] loss:9.0295 || sec:5.9175\n",
      "[2090] loss:9.0591 || sec:6.0926\n",
      "[2100] loss:9.6504 || sec:6.2542\n",
      "[2110] loss:9.4883 || sec:6.0250\n",
      "[2120] loss:9.6221 || sec:6.0682\n",
      "[2130] loss:8.3909 || sec:6.1916\n",
      "[2140] loss:9.3462 || sec:6.0218\n",
      "[12] train_loss: 1717.1289 || val_loss: 0.0000 || sec:113.5379\n",
      "Epoch 13/50\n",
      "[train]\n",
      "[2150] loss:8.3206 || sec:1.0605\n",
      "[2160] loss:9.3671 || sec:6.0035\n",
      "[2170] loss:8.0852 || sec:6.3619\n",
      "[2180] loss:9.4256 || sec:6.2769\n",
      "[2190] loss:9.0772 || sec:6.4258\n",
      "[2200] loss:8.1991 || sec:6.4743\n",
      "[2210] loss:11.2432 || sec:6.6187\n",
      "[2220] loss:10.5127 || sec:6.4370\n",
      "[2230] loss:8.9081 || sec:6.5203\n",
      "[2240] loss:8.3198 || sec:5.9932\n",
      "[2250] loss:7.7632 || sec:6.4502\n",
      "[2260] loss:10.3528 || sec:6.1814\n",
      "[2270] loss:8.3229 || sec:6.1594\n",
      "[2280] loss:8.5419 || sec:6.7602\n",
      "[2290] loss:8.7072 || sec:6.1740\n",
      "[2300] loss:9.3189 || sec:6.3397\n",
      "[2310] loss:10.7202 || sec:6.5009\n",
      "[2320] loss:11.4799 || sec:6.1725\n",
      "[13] train_loss: 1672.1518 || val_loss: 0.0000 || sec:115.8428\n",
      "Epoch 14/50\n",
      "[train]\n",
      "[2330] loss:10.0602 || sec:1.8305\n",
      "[2340] loss:8.5861 || sec:6.7758\n",
      "[2350] loss:10.2161 || sec:6.3710\n",
      "[2360] loss:8.0364 || sec:6.4018\n",
      "[2370] loss:8.9766 || sec:6.1532\n",
      "[2380] loss:9.3764 || sec:6.2337\n",
      "[2390] loss:7.9303 || sec:6.1664\n",
      "[2400] loss:8.2870 || sec:6.4114\n",
      "[2410] loss:9.4053 || sec:6.5502\n",
      "[2420] loss:8.2899 || sec:6.4704\n",
      "[2430] loss:7.8404 || sec:6.5990\n",
      "[2440] loss:9.0941 || sec:6.2888\n",
      "[2450] loss:8.2055 || sec:6.5606\n",
      "[2460] loss:8.1591 || sec:6.2586\n",
      "[2470] loss:9.4137 || sec:6.2874\n",
      "[2480] loss:10.8991 || sec:6.5139\n",
      "[2490] loss:7.7115 || sec:6.1593\n",
      "[2500] loss:8.3232 || sec:6.4105\n",
      "[14] train_loss: 1582.6271 || val_loss: 0.0000 || sec:117.0922\n",
      "Epoch 15/50\n",
      "[train]\n",
      "[2510] loss:8.9718 || sec:2.5294\n",
      "[2520] loss:7.8233 || sec:6.4206\n",
      "[2530] loss:10.4878 || sec:6.5071\n",
      "[2540] loss:8.2001 || sec:5.8559\n",
      "[2550] loss:8.4236 || sec:6.1906\n",
      "[2560] loss:9.5484 || sec:6.1296\n",
      "[2570] loss:8.1689 || sec:5.8359\n",
      "[2580] loss:8.7437 || sec:6.0235\n",
      "[2590] loss:10.4204 || sec:6.5178\n",
      "[2600] loss:9.2385 || sec:6.8293\n",
      "[2610] loss:8.9564 || sec:6.1393\n",
      "[2620] loss:8.6709 || sec:6.0031\n",
      "[2630] loss:8.0479 || sec:6.9345\n",
      "[2640] loss:8.1948 || sec:6.2158\n",
      "[2650] loss:9.5189 || sec:6.3526\n",
      "[2660] loss:8.4771 || sec:5.8451\n",
      "[2670] loss:7.5467 || sec:6.3655\n",
      "[2680] loss:10.4318 || sec:6.4732\n",
      "[15] train_loss: 1561.6936 || val_loss: 0.0000 || sec:114.8182\n",
      "Epoch 16/50\n",
      "[train]\n",
      "[2690] loss:8.5593 || sec:3.1596\n",
      "[2700] loss:7.3900 || sec:6.2189\n",
      "[2710] loss:7.6989 || sec:6.5814\n",
      "[2720] loss:8.6318 || sec:6.2356\n",
      "[2730] loss:9.5309 || sec:6.1508\n",
      "[2740] loss:8.2931 || sec:6.1183\n",
      "[2750] loss:8.1190 || sec:6.5587\n",
      "[2760] loss:7.9209 || sec:6.2713\n",
      "[2770] loss:8.9320 || sec:5.8370\n",
      "[2780] loss:9.8183 || sec:6.0032\n",
      "[2790] loss:9.5437 || sec:5.8771\n",
      "[2800] loss:8.1917 || sec:6.0000\n",
      "[2810] loss:8.9398 || sec:5.8896\n",
      "[2820] loss:7.8461 || sec:6.0112\n",
      "[2830] loss:6.8940 || sec:5.9203\n",
      "[2840] loss:8.3816 || sec:5.9136\n",
      "[2850] loss:9.0139 || sec:5.6577\n",
      "[2860] loss:7.8503 || sec:5.9688\n",
      "[16] train_loss: 1494.1536 || val_loss: 0.0000 || sec:111.2355\n",
      "Epoch 17/50\n",
      "[train]\n",
      "[2870] loss:8.1772 || sec:3.4901\n",
      "[2880] loss:10.0421 || sec:5.9195\n",
      "[2890] loss:8.2555 || sec:6.2090\n",
      "[2900] loss:8.3642 || sec:5.9311\n",
      "[2910] loss:8.6266 || sec:5.8067\n",
      "[2920] loss:7.9956 || sec:5.7872\n",
      "[2930] loss:7.2426 || sec:6.3136\n",
      "[2940] loss:7.5155 || sec:5.8485\n",
      "[2950] loss:9.2004 || sec:5.9922\n",
      "[2960] loss:8.0714 || sec:5.6671\n",
      "[2970] loss:9.2341 || sec:6.0245\n",
      "[2980] loss:8.1339 || sec:5.9132\n",
      "[2990] loss:8.1683 || sec:6.2142\n",
      "[3000] loss:10.0938 || sec:6.0731\n",
      "[3010] loss:9.3585 || sec:6.1220\n",
      "[3020] loss:9.1657 || sec:5.9596\n",
      "[3030] loss:7.8417 || sec:6.1574\n",
      "[3040] loss:8.7784 || sec:6.2573\n",
      "[17] train_loss: 1549.1174 || val_loss: 0.0000 || sec:110.0466\n",
      "Epoch 18/50\n",
      "[train]\n",
      "[3050] loss:7.2771 || sec:4.6429\n",
      "[3060] loss:8.4511 || sec:6.8704\n",
      "[3070] loss:7.8336 || sec:6.4269\n",
      "[3080] loss:7.6114 || sec:6.2791\n",
      "[3090] loss:7.3717 || sec:6.4487\n",
      "[3100] loss:9.4145 || sec:6.3758\n",
      "[3110] loss:7.4097 || sec:6.3692\n",
      "[3120] loss:8.7971 || sec:6.5700\n",
      "[3130] loss:10.1052 || sec:6.4347\n",
      "[3140] loss:7.6069 || sec:5.9574\n",
      "[3150] loss:8.9648 || sec:6.1793\n",
      "[3160] loss:9.7618 || sec:5.9267\n",
      "[3170] loss:8.7493 || sec:5.9685\n",
      "[3180] loss:8.1864 || sec:6.3451\n",
      "[3190] loss:8.6293 || sec:6.1109\n",
      "[3200] loss:7.4810 || sec:5.9528\n",
      "[3210] loss:8.4608 || sec:5.8964\n",
      "[3220] loss:8.9409 || sec:5.7800\n",
      "[18] train_loss: 1495.8695 || val_loss: 0.0000 || sec:114.2011\n",
      "Epoch 19/50\n",
      "[train]\n",
      "[3230] loss:8.5049 || sec:4.6597\n",
      "[3240] loss:8.2042 || sec:5.8079\n",
      "[3250] loss:7.9793 || sec:5.8533\n",
      "[3260] loss:10.6619 || sec:5.9135\n",
      "[3270] loss:9.8854 || sec:5.9324\n",
      "[3280] loss:7.5259 || sec:6.0486\n",
      "[3290] loss:8.5111 || sec:5.8455\n",
      "[3300] loss:8.2098 || sec:6.1867\n",
      "[3310] loss:6.8592 || sec:5.9496\n",
      "[3320] loss:8.7170 || sec:6.1211\n",
      "[3330] loss:7.3431 || sec:5.7927\n",
      "[3340] loss:8.8278 || sec:5.8364\n",
      "[3350] loss:8.2595 || sec:6.1545\n",
      "[3360] loss:8.4087 || sec:6.0816\n",
      "[3370] loss:8.7992 || sec:6.0341\n",
      "[3380] loss:9.0832 || sec:5.8254\n",
      "[3390] loss:8.2263 || sec:5.8689\n",
      "[3400] loss:7.1954 || sec:5.7914\n",
      "[19] train_loss: 1507.1960 || val_loss: 0.0000 || sec:108.5984\n",
      "Epoch 20/50\n",
      "[train]\n",
      "[3410] loss:8.0843 || sec:5.7673\n",
      "[3420] loss:8.9256 || sec:5.9653\n",
      "[3430] loss:6.7960 || sec:5.7959\n",
      "[3440] loss:8.3862 || sec:6.0133\n",
      "[3450] loss:7.0815 || sec:6.2005\n",
      "[3460] loss:7.0862 || sec:6.0319\n",
      "[3470] loss:8.2240 || sec:6.0186\n",
      "[3480] loss:8.1142 || sec:6.1044\n",
      "[3490] loss:9.2186 || sec:6.1458\n",
      "[3500] loss:7.9077 || sec:6.0540\n",
      "[3510] loss:8.6818 || sec:5.9705\n",
      "[3520] loss:8.0365 || sec:6.0013\n",
      "[3530] loss:7.6023 || sec:6.1111\n",
      "[3540] loss:9.2083 || sec:6.0585\n",
      "[3550] loss:8.3951 || sec:6.4068\n",
      "[3560] loss:8.5437 || sec:6.1387\n",
      "[3570] loss:7.1491 || sec:5.7505\n",
      "[3580] loss:7.4210 || sec:5.7616\n",
      "[val]\n",
      "[20] train_loss: 1435.2623 || val_loss: 1320.5453 || sec:157.7696\n",
      "Epoch 21/50\n",
      "[train]\n",
      "[3590] loss:7.0692 || sec:6.0177\n",
      "[3600] loss:7.1383 || sec:6.0486\n",
      "[3610] loss:6.8998 || sec:6.0638\n",
      "[3620] loss:6.7327 || sec:6.1390\n",
      "[3630] loss:6.7680 || sec:5.8446\n",
      "[3640] loss:6.8794 || sec:6.1622\n",
      "[3650] loss:6.5831 || sec:6.2938\n",
      "[3660] loss:7.1802 || sec:5.9704\n",
      "[3670] loss:6.9762 || sec:5.8487\n",
      "[3680] loss:6.7103 || sec:6.1819\n",
      "[3690] loss:7.7464 || sec:6.3067\n",
      "[3700] loss:6.4521 || sec:6.4908\n",
      "[3710] loss:6.4920 || sec:6.2996\n",
      "[3720] loss:6.6702 || sec:6.1469\n",
      "[3730] loss:7.1769 || sec:6.4606\n",
      "[3740] loss:6.5928 || sec:6.4945\n",
      "[3750] loss:6.6852 || sec:6.4699\n",
      "[21] train_loss: 1248.2357 || val_loss: 0.0000 || sec:113.0928\n",
      "Epoch 22/50\n",
      "[train]\n",
      "[3760] loss:7.7792 || sec:0.4774\n",
      "[3770] loss:6.3181 || sec:5.9920\n",
      "[3780] loss:6.9568 || sec:6.1279\n",
      "[3790] loss:7.5056 || sec:5.8701\n",
      "[3800] loss:7.0603 || sec:6.0747\n",
      "[3810] loss:6.7608 || sec:6.3337\n",
      "[3820] loss:7.1539 || sec:6.1095\n",
      "[3830] loss:6.7716 || sec:6.2295\n",
      "[3840] loss:6.9909 || sec:6.4492\n",
      "[3850] loss:6.7628 || sec:6.0664\n",
      "[3860] loss:6.2569 || sec:6.4481\n",
      "[3870] loss:6.9434 || sec:6.3149\n",
      "[3880] loss:6.9356 || sec:6.2194\n",
      "[3890] loss:6.9340 || sec:6.3741\n",
      "[3900] loss:6.6885 || sec:6.2831\n",
      "[3910] loss:6.6467 || sec:6.0299\n",
      "[3920] loss:7.0143 || sec:6.0178\n",
      "[3930] loss:7.8667 || sec:6.0120\n",
      "[22] train_loss: 1249.1300 || val_loss: 0.0000 || sec:112.6432\n",
      "Epoch 23/50\n",
      "[train]\n",
      "[3940] loss:7.5073 || sec:1.0562\n",
      "[3950] loss:7.3442 || sec:6.1160\n",
      "[3960] loss:7.3601 || sec:6.3093\n",
      "[3970] loss:6.5966 || sec:6.2431\n",
      "[3980] loss:6.9471 || sec:6.2997\n",
      "[3990] loss:6.7396 || sec:6.2567\n",
      "[4000] loss:6.6089 || sec:6.1860\n",
      "[4010] loss:7.2218 || sec:6.9900\n",
      "[4020] loss:8.7080 || sec:6.3020\n",
      "[4030] loss:8.2547 || sec:7.5718\n",
      "[4040] loss:7.5234 || sec:6.8297\n",
      "[4050] loss:7.4086 || sec:7.8838\n",
      "[4060] loss:7.6382 || sec:7.3446\n",
      "[4070] loss:7.2680 || sec:6.8391\n",
      "[4080] loss:6.9881 || sec:7.0795\n",
      "[4090] loss:6.7177 || sec:7.5532\n",
      "[4100] loss:6.7422 || sec:7.1605\n",
      "[4110] loss:6.7366 || sec:6.6782\n",
      "[23] train_loss: 1262.6961 || val_loss: 0.0000 || sec:124.0997\n",
      "Epoch 24/50\n",
      "[train]\n",
      "[4120] loss:6.7940 || sec:1.7514\n",
      "[4130] loss:9.2251 || sec:6.5076\n",
      "[4140] loss:6.5147 || sec:6.9481\n",
      "[4150] loss:7.2787 || sec:6.7221\n",
      "[4160] loss:6.6276 || sec:6.7139\n",
      "[4170] loss:6.6335 || sec:6.7422\n",
      "[4180] loss:7.0335 || sec:6.5068\n",
      "[4190] loss:6.9332 || sec:6.3413\n",
      "[4200] loss:6.7647 || sec:6.8497\n",
      "[4210] loss:6.3747 || sec:6.0544\n",
      "[4220] loss:6.7457 || sec:6.2826\n",
      "[4230] loss:6.5874 || sec:6.3649\n",
      "[4240] loss:6.4639 || sec:5.9950\n",
      "[4250] loss:6.6208 || sec:6.1922\n",
      "[4260] loss:6.6843 || sec:6.4682\n",
      "[4270] loss:6.5154 || sec:6.1586\n",
      "[4280] loss:6.7376 || sec:6.0326\n",
      "[4290] loss:6.3280 || sec:6.0754\n",
      "[24] train_loss: 1253.2042 || val_loss: 0.0000 || sec:116.8723\n",
      "Epoch 25/50\n",
      "[train]\n",
      "[4300] loss:7.2135 || sec:2.3536\n",
      "[4310] loss:6.8987 || sec:6.6736\n",
      "[4320] loss:6.9506 || sec:6.2964\n",
      "[4330] loss:6.8254 || sec:6.4880\n",
      "[4340] loss:7.1951 || sec:6.3210\n",
      "[4350] loss:6.7227 || sec:6.4418\n",
      "[4360] loss:6.9272 || sec:6.5020\n",
      "[4370] loss:6.8360 || sec:6.1497\n",
      "[4380] loss:6.9425 || sec:6.1205\n",
      "[4390] loss:6.2574 || sec:5.7738\n",
      "[4400] loss:6.5647 || sec:6.0368\n",
      "[4410] loss:6.3594 || sec:5.8346\n",
      "[4420] loss:6.6145 || sec:5.9152\n",
      "[4430] loss:6.6113 || sec:5.9735\n",
      "[4440] loss:6.5094 || sec:6.2336\n",
      "[4450] loss:6.6582 || sec:6.3821\n",
      "[4460] loss:6.1591 || sec:6.0218\n",
      "[4470] loss:6.3886 || sec:6.0339\n",
      "[25] train_loss: 1197.9196 || val_loss: 0.0000 || sec:112.8391\n",
      "Epoch 26/50\n",
      "[train]\n",
      "[4480] loss:6.3683 || sec:2.9041\n",
      "[4490] loss:6.7420 || sec:6.0188\n",
      "[4500] loss:6.5501 || sec:6.2434\n",
      "[4510] loss:6.5042 || sec:5.8664\n",
      "[4520] loss:6.3482 || sec:5.8367\n",
      "[4530] loss:6.3916 || sec:6.1196\n",
      "[4540] loss:6.3359 || sec:6.2438\n",
      "[4550] loss:6.8521 || sec:6.1229\n",
      "[4560] loss:6.4321 || sec:6.2868\n",
      "[4570] loss:6.9704 || sec:6.1971\n",
      "[4580] loss:6.4757 || sec:6.1710\n",
      "[4590] loss:6.2745 || sec:5.9615\n",
      "[4600] loss:6.5604 || sec:5.9926\n",
      "[4610] loss:6.4855 || sec:6.0494\n",
      "[4620] loss:6.2474 || sec:5.6479\n",
      "[4630] loss:6.1582 || sec:6.0126\n",
      "[4640] loss:6.5573 || sec:6.2260\n",
      "[4650] loss:5.8586 || sec:6.0920\n",
      "[26] train_loss: 1151.7805 || val_loss: 0.0000 || sec:110.9017\n",
      "Epoch 27/50\n",
      "[train]\n",
      "[4660] loss:6.7471 || sec:3.6711\n",
      "[4670] loss:6.2050 || sec:6.4759\n",
      "[4680] loss:6.8537 || sec:6.2208\n",
      "[4690] loss:6.8972 || sec:6.2007\n",
      "[4700] loss:6.4463 || sec:6.1245\n",
      "[4710] loss:6.6291 || sec:6.1751\n",
      "[4720] loss:6.1634 || sec:6.4482\n",
      "[4730] loss:7.1856 || sec:6.3045\n",
      "[4740] loss:7.5964 || sec:5.9977\n",
      "[4750] loss:7.3310 || sec:6.1965\n",
      "[4760] loss:6.8559 || sec:6.2108\n",
      "[4770] loss:6.6454 || sec:6.9464\n",
      "[4780] loss:6.1802 || sec:6.5282\n",
      "[4790] loss:6.2698 || sec:6.3513\n",
      "[4800] loss:6.3695 || sec:6.4836\n",
      "[4810] loss:6.6261 || sec:6.7002\n",
      "[4820] loss:6.3062 || sec:7.1940\n",
      "[4830] loss:5.9701 || sec:6.7099\n",
      "[27] train_loss: 1178.0819 || val_loss: 0.0000 || sec:117.4359\n",
      "Epoch 28/50\n",
      "[train]\n",
      "[4840] loss:6.8110 || sec:4.6925\n",
      "[4850] loss:6.3691 || sec:6.5131\n",
      "[4860] loss:6.1242 || sec:6.8034\n",
      "[4870] loss:6.4461 || sec:6.6633\n",
      "[4880] loss:6.0932 || sec:6.5275\n",
      "[4890] loss:6.7996 || sec:6.3271\n",
      "[4900] loss:6.4754 || sec:5.9657\n",
      "[4910] loss:6.8069 || sec:5.8628\n",
      "[4920] loss:6.5966 || sec:6.2206\n",
      "[4930] loss:6.4333 || sec:6.0713\n",
      "[4940] loss:6.2279 || sec:6.1901\n",
      "[4950] loss:6.0468 || sec:6.2511\n",
      "[4960] loss:6.3765 || sec:6.3391\n",
      "[4970] loss:6.0061 || sec:6.3722\n",
      "[4980] loss:6.3205 || sec:6.0690\n",
      "[4990] loss:6.4102 || sec:6.4745\n",
      "[5000] loss:6.2468 || sec:6.2577\n",
      "[5010] loss:6.3714 || sec:6.1720\n",
      "[28] train_loss: 1141.2632 || val_loss: 0.0000 || sec:115.4953\n",
      "Epoch 29/50\n",
      "[train]\n",
      "[5020] loss:6.7227 || sec:4.8885\n",
      "[5030] loss:6.2926 || sec:6.1674\n",
      "[5040] loss:5.9355 || sec:6.4742\n",
      "[5050] loss:6.0352 || sec:6.3889\n",
      "[5060] loss:6.4485 || sec:6.2138\n",
      "[5070] loss:6.4263 || sec:6.0995\n",
      "[5080] loss:5.8925 || sec:6.1231\n",
      "[5090] loss:6.2532 || sec:6.3463\n",
      "[5100] loss:6.4328 || sec:6.3946\n",
      "[5110] loss:6.3492 || sec:6.1112\n",
      "[5120] loss:5.8708 || sec:6.0984\n",
      "[5130] loss:6.4147 || sec:5.7912\n",
      "[5140] loss:6.4548 || sec:6.0976\n",
      "[5150] loss:6.4295 || sec:6.2030\n",
      "[5160] loss:6.3990 || sec:6.1410\n",
      "[5170] loss:6.3875 || sec:6.2196\n",
      "[5180] loss:6.4499 || sec:6.0426\n",
      "[5190] loss:6.2280 || sec:5.8313\n",
      "[29] train_loss: 1132.7549 || val_loss: 0.0000 || sec:112.6074\n",
      "Epoch 30/50\n",
      "[train]\n",
      "[5200] loss:5.9958 || sec:5.4692\n",
      "[5210] loss:5.9129 || sec:5.9058\n",
      "[5220] loss:5.9313 || sec:6.1330\n",
      "[5230] loss:6.2683 || sec:5.9819\n",
      "[5240] loss:6.2191 || sec:6.0820\n",
      "[5250] loss:6.0917 || sec:5.8970\n",
      "[5260] loss:5.9269 || sec:5.8741\n",
      "[5270] loss:6.0917 || sec:6.0035\n",
      "[5280] loss:6.1568 || sec:5.9439\n",
      "[5290] loss:5.6818 || sec:5.4697\n",
      "[5300] loss:6.2021 || sec:5.8963\n",
      "[5310] loss:6.2224 || sec:5.9725\n",
      "[5320] loss:6.2765 || sec:5.9816\n",
      "[5330] loss:6.5598 || sec:5.8746\n",
      "[5340] loss:6.2362 || sec:5.9708\n",
      "[5350] loss:6.2904 || sec:5.7069\n",
      "[5360] loss:6.4248 || sec:5.9570\n",
      "[5370] loss:6.5388 || sec:5.9382\n",
      "[val]\n",
      "[30] train_loss: 1112.1220 || val_loss: 1126.8094 || sec:152.7708\n",
      "Epoch 31/50\n",
      "[train]\n",
      "[5380] loss:6.0220 || sec:5.9777\n",
      "[5390] loss:5.8583 || sec:5.8075\n",
      "[5400] loss:6.4722 || sec:5.8033\n",
      "[5410] loss:6.2921 || sec:5.9015\n",
      "[5420] loss:6.0008 || sec:5.7827\n",
      "[5430] loss:6.1904 || sec:5.8783\n",
      "[5440] loss:5.5602 || sec:5.8462\n",
      "[5450] loss:6.4028 || sec:5.9336\n",
      "[5460] loss:5.7969 || sec:5.6937\n",
      "[5470] loss:5.9703 || sec:6.0131\n",
      "[5480] loss:6.7200 || sec:6.0612\n",
      "[5490] loss:6.4707 || sec:6.0203\n",
      "[5500] loss:6.0142 || sec:6.0734\n",
      "[5510] loss:6.3795 || sec:5.6303\n",
      "[5520] loss:6.2950 || sec:6.0791\n",
      "[5530] loss:6.0207 || sec:6.1483\n",
      "[5540] loss:6.2583 || sec:5.8112\n",
      "[31] train_loss: 1112.4779 || val_loss: 0.0000 || sec:107.8688\n",
      "Epoch 32/50\n",
      "[train]\n",
      "[5550] loss:6.0128 || sec:0.4809\n",
      "[5560] loss:5.8115 || sec:5.8564\n",
      "[5570] loss:6.4119 || sec:5.9715\n",
      "[5580] loss:5.9746 || sec:5.9978\n",
      "[5590] loss:5.6541 || sec:5.9077\n",
      "[5600] loss:5.7756 || sec:5.8977\n",
      "[5610] loss:5.8607 || sec:5.9990\n",
      "[5620] loss:6.1679 || sec:6.2734\n",
      "[5630] loss:6.2787 || sec:5.8725\n",
      "[5640] loss:5.4657 || sec:5.8404\n",
      "[5650] loss:5.4735 || sec:5.8168\n",
      "[5660] loss:5.9155 || sec:6.0307\n",
      "[5670] loss:5.4697 || sec:5.9321\n",
      "[5680] loss:6.0838 || sec:6.2214\n",
      "[5690] loss:5.9103 || sec:6.2722\n",
      "[5700] loss:5.4672 || sec:5.7908\n",
      "[5710] loss:6.0354 || sec:6.0115\n",
      "[5720] loss:6.0826 || sec:5.9087\n",
      "[32] train_loss: 1080.2709 || val_loss: 0.0000 || sec:109.2389\n",
      "Epoch 33/50\n",
      "[train]\n",
      "[5730] loss:5.9542 || sec:1.2013\n",
      "[5740] loss:5.5740 || sec:6.1848\n",
      "[5750] loss:5.6287 || sec:5.7984\n",
      "[5760] loss:6.2556 || sec:6.1462\n",
      "[5770] loss:6.1349 || sec:5.9340\n",
      "[5780] loss:5.2981 || sec:5.9810\n",
      "[5790] loss:5.6248 || sec:5.7441\n",
      "[5800] loss:6.4433 || sec:5.9230\n",
      "[5810] loss:5.8409 || sec:5.8917\n",
      "[5820] loss:5.7056 || sec:6.1268\n",
      "[5830] loss:6.1958 || sec:6.0044\n",
      "[5840] loss:6.1936 || sec:5.8815\n",
      "[5850] loss:5.9854 || sec:5.9505\n",
      "[5860] loss:5.8964 || sec:5.9108\n",
      "[5870] loss:5.4893 || sec:5.9144\n",
      "[5880] loss:6.1988 || sec:5.9872\n",
      "[5890] loss:6.2366 || sec:5.9926\n",
      "[5900] loss:5.8006 || sec:5.8499\n",
      "[33] train_loss: 1072.3538 || val_loss: 0.0000 || sec:108.7900\n",
      "Epoch 34/50\n",
      "[train]\n",
      "[5910] loss:6.5899 || sec:1.7174\n",
      "[5920] loss:6.3608 || sec:5.7093\n",
      "[5930] loss:6.0451 || sec:6.0221\n",
      "[5940] loss:6.1126 || sec:5.6980\n",
      "[5950] loss:6.2641 || sec:6.0918\n",
      "[5960] loss:6.1960 || sec:5.7259\n",
      "[5970] loss:6.1380 || sec:5.9611\n",
      "[5980] loss:5.8432 || sec:5.8838\n",
      "[5990] loss:6.4231 || sec:5.9981\n",
      "[6000] loss:5.9478 || sec:5.7410\n",
      "[6010] loss:6.4422 || sec:5.8180\n",
      "[6020] loss:5.7701 || sec:6.2856\n",
      "[6030] loss:5.7445 || sec:5.8670\n",
      "[6040] loss:5.8064 || sec:5.7559\n",
      "[6050] loss:5.9199 || sec:6.0509\n",
      "[6060] loss:5.9928 || sec:6.0176\n",
      "[6070] loss:5.9421 || sec:5.9533\n",
      "[6080] loss:5.8292 || sec:5.9973\n",
      "[34] train_loss: 1070.6293 || val_loss: 0.0000 || sec:108.3497\n",
      "Epoch 35/50\n",
      "[train]\n",
      "[6090] loss:5.7002 || sec:2.4469\n",
      "[6100] loss:6.0089 || sec:5.7863\n",
      "[6110] loss:5.5348 || sec:5.8596\n",
      "[6120] loss:5.4029 || sec:5.5547\n",
      "[6130] loss:5.9045 || sec:6.1212\n",
      "[6140] loss:5.7118 || sec:5.8832\n",
      "[6150] loss:5.7714 || sec:5.9142\n",
      "[6160] loss:6.1017 || sec:5.9590\n",
      "[6170] loss:6.1288 || sec:6.1088\n",
      "[6180] loss:5.7091 || sec:5.8727\n",
      "[6190] loss:6.0178 || sec:5.8774\n",
      "[6200] loss:5.8445 || sec:6.0083\n",
      "[6210] loss:6.0174 || sec:5.8311\n",
      "[6220] loss:5.7438 || sec:6.0027\n",
      "[6230] loss:5.7239 || sec:5.7626\n",
      "[6240] loss:5.9342 || sec:5.9518\n",
      "[6250] loss:5.7083 || sec:5.9845\n",
      "[6260] loss:5.5305 || sec:5.8081\n",
      "[35] train_loss: 1029.9236 || val_loss: 0.0000 || sec:108.0239\n",
      "Epoch 36/50\n",
      "[train]\n",
      "[6270] loss:5.0984 || sec:2.8903\n",
      "[6280] loss:5.4335 || sec:5.7652\n",
      "[6290] loss:6.2817 || sec:5.7877\n",
      "[6300] loss:5.3395 || sec:5.8860\n",
      "[6310] loss:6.1638 || sec:6.1498\n",
      "[6320] loss:5.5865 || sec:5.7671\n",
      "[6330] loss:5.3986 || sec:5.9685\n",
      "[6340] loss:5.6395 || sec:6.0036\n",
      "[6350] loss:5.8844 || sec:6.2095\n",
      "[6360] loss:5.5587 || sec:5.7536\n",
      "[6370] loss:5.5413 || sec:5.7930\n",
      "[6380] loss:5.4947 || sec:5.9908\n",
      "[6390] loss:5.8834 || sec:5.9996\n",
      "[6400] loss:5.7006 || sec:6.0028\n",
      "[6410] loss:5.6053 || sec:6.0182\n",
      "[6420] loss:5.3328 || sec:5.9498\n",
      "[6430] loss:6.2441 || sec:5.9329\n",
      "[6440] loss:6.1508 || sec:6.0064\n",
      "[36] train_loss: 1024.2347 || val_loss: 0.0000 || sec:108.8278\n",
      "Epoch 37/50\n",
      "[train]\n",
      "[6450] loss:5.9931 || sec:3.3600\n",
      "[6460] loss:5.6089 || sec:5.7749\n",
      "[6470] loss:5.7290 || sec:5.8019\n",
      "[6480] loss:5.2724 || sec:6.0487\n",
      "[6490] loss:5.6629 || sec:5.7939\n",
      "[6500] loss:5.6491 || sec:5.9081\n",
      "[6510] loss:5.7867 || sec:5.9006\n",
      "[6520] loss:5.8064 || sec:5.9012\n",
      "[6530] loss:5.5792 || sec:5.7729\n",
      "[6540] loss:5.7526 || sec:6.0180\n",
      "[6550] loss:5.5325 || sec:5.7855\n",
      "[6560] loss:5.6268 || sec:5.7771\n",
      "[6570] loss:5.6227 || sec:6.0133\n",
      "[6580] loss:5.4825 || sec:6.0260\n",
      "[6590] loss:6.0616 || sec:6.1112\n",
      "[6600] loss:5.6415 || sec:5.9643\n",
      "[6610] loss:5.9412 || sec:5.7908\n",
      "[6620] loss:5.3045 || sec:5.7938\n",
      "[37] train_loss: 1009.8394 || val_loss: 0.0000 || sec:107.4821\n",
      "Epoch 38/50\n",
      "[train]\n",
      "[6630] loss:5.8492 || sec:4.2700\n",
      "[6640] loss:5.4762 || sec:5.8682\n",
      "[6650] loss:5.5078 || sec:6.1523\n",
      "[6660] loss:5.7485 || sec:6.0648\n",
      "[6670] loss:5.6389 || sec:5.8202\n",
      "[6680] loss:6.0503 || sec:6.0576\n",
      "[6690] loss:5.7593 || sec:5.9432\n",
      "[6700] loss:5.8201 || sec:6.2077\n",
      "[6710] loss:5.6142 || sec:5.9771\n",
      "[6720] loss:6.0131 || sec:5.9423\n",
      "[6730] loss:5.4112 || sec:5.9358\n",
      "[6740] loss:5.3246 || sec:6.0262\n",
      "[6750] loss:5.4462 || sec:5.7871\n",
      "[6760] loss:5.3954 || sec:5.9959\n",
      "[6770] loss:5.8703 || sec:5.8378\n",
      "[6780] loss:5.2964 || sec:5.7488\n",
      "[6790] loss:5.5426 || sec:5.9585\n",
      "[6800] loss:5.6389 || sec:5.8762\n",
      "[38] train_loss: 996.8864 || val_loss: 0.0000 || sec:109.0086\n",
      "Epoch 39/50\n",
      "[train]\n",
      "[6810] loss:5.4890 || sec:4.5911\n",
      "[6820] loss:5.5374 || sec:5.7503\n",
      "[6830] loss:5.2578 || sec:5.8239\n",
      "[6840] loss:5.8005 || sec:6.0788\n",
      "[6850] loss:5.4253 || sec:5.8474\n",
      "[6860] loss:5.8623 || sec:6.0367\n",
      "[6870] loss:5.4049 || sec:5.8023\n",
      "[6880] loss:5.6629 || sec:6.1618\n",
      "[6890] loss:5.4109 || sec:5.8996\n",
      "[6900] loss:5.6468 || sec:5.9513\n",
      "[6910] loss:5.1443 || sec:5.8963\n",
      "[6920] loss:5.7213 || sec:5.9233\n",
      "[6930] loss:5.7171 || sec:5.8189\n",
      "[6940] loss:6.0338 || sec:5.8211\n",
      "[6950] loss:5.5807 || sec:5.9168\n",
      "[6960] loss:5.5029 || sec:5.7644\n",
      "[6970] loss:6.1427 || sec:5.7811\n",
      "[6980] loss:5.5208 || sec:5.7634\n",
      "[39] train_loss: 977.8360 || val_loss: 0.0000 || sec:107.5085\n",
      "Epoch 40/50\n",
      "[train]\n",
      "[6990] loss:5.3892 || sec:5.4436\n",
      "[7000] loss:5.3988 || sec:5.8142\n",
      "[7010] loss:5.2729 || sec:5.9946\n",
      "[7020] loss:5.6251 || sec:5.8807\n",
      "[7030] loss:5.5136 || sec:5.5928\n",
      "[7040] loss:5.4994 || sec:5.9034\n",
      "[7050] loss:5.7053 || sec:5.8307\n",
      "[7060] loss:5.3943 || sec:5.7911\n",
      "[7070] loss:5.1133 || sec:5.8285\n",
      "[7080] loss:5.3835 || sec:6.0570\n",
      "[7090] loss:4.8687 || sec:5.7205\n",
      "[7100] loss:5.5137 || sec:5.8294\n",
      "[7110] loss:5.0960 || sec:5.6975\n",
      "[7120] loss:5.4057 || sec:6.0862\n",
      "[7130] loss:5.3397 || sec:5.6155\n",
      "[7140] loss:5.5845 || sec:5.8097\n",
      "[7150] loss:5.2535 || sec:5.7844\n",
      "[7160] loss:5.8263 || sec:5.5037\n",
      "[val]\n",
      "[40] train_loss: 976.2090 || val_loss: 993.4047 || sec:151.2463\n",
      "Epoch 41/50\n",
      "[train]\n",
      "[7170] loss:5.7415 || sec:6.0599\n",
      "[7180] loss:5.6909 || sec:6.0978\n",
      "[7190] loss:5.0107 || sec:5.9488\n",
      "[7200] loss:6.0310 || sec:6.2230\n",
      "[7210] loss:5.0591 || sec:5.7672\n",
      "[7220] loss:5.2243 || sec:5.9193\n",
      "[7230] loss:5.3786 || sec:5.8671\n",
      "[7240] loss:5.4096 || sec:5.9305\n",
      "[7250] loss:5.1480 || sec:5.8740\n",
      "[7260] loss:5.3070 || sec:5.8241\n",
      "[7270] loss:5.6755 || sec:5.9688\n",
      "[7280] loss:5.3517 || sec:5.8203\n",
      "[7290] loss:4.9527 || sec:5.8493\n",
      "[7300] loss:6.2740 || sec:6.0954\n",
      "[7310] loss:4.9378 || sec:5.8824\n",
      "[7320] loss:5.3039 || sec:5.7648\n",
      "[7330] loss:5.5348 || sec:5.8231\n",
      "[41] train_loss: 958.4334 || val_loss: 0.0000 || sec:108.3588\n",
      "Epoch 42/50\n",
      "[train]\n",
      "[7340] loss:5.4321 || sec:0.4226\n",
      "[7350] loss:5.7519 || sec:5.9848\n",
      "[7360] loss:5.6057 || sec:5.9286\n",
      "[7370] loss:5.3069 || sec:5.8994\n",
      "[7380] loss:5.2975 || sec:5.7466\n",
      "[7390] loss:5.1359 || sec:5.6422\n",
      "[7400] loss:4.9074 || sec:5.8407\n",
      "[7410] loss:5.1010 || sec:5.8051\n",
      "[7420] loss:5.3547 || sec:5.8463\n",
      "[7430] loss:5.0144 || sec:5.8083\n",
      "[7440] loss:5.4153 || sec:6.0159\n",
      "[7450] loss:5.0898 || sec:6.0926\n",
      "[7460] loss:4.9134 || sec:5.9265\n",
      "[7470] loss:5.2875 || sec:5.9794\n",
      "[7480] loss:5.1778 || sec:5.7968\n",
      "[7490] loss:5.9758 || sec:6.0335\n",
      "[7500] loss:5.4136 || sec:5.7877\n",
      "[7510] loss:4.9236 || sec:5.7657\n",
      "[42] train_loss: 951.1239 || val_loss: 0.0000 || sec:107.5836\n",
      "Epoch 43/50\n",
      "[train]\n",
      "[7520] loss:5.9351 || sec:0.9481\n",
      "[7530] loss:5.1110 || sec:5.8642\n",
      "[7540] loss:5.1343 || sec:5.9847\n",
      "[7550] loss:4.9648 || sec:5.9586\n",
      "[7560] loss:5.1413 || sec:6.0252\n",
      "[7570] loss:5.8785 || sec:6.0509\n",
      "[7580] loss:5.5256 || sec:6.1496\n",
      "[7590] loss:5.5266 || sec:6.0055\n",
      "[7600] loss:5.3045 || sec:5.9178\n",
      "[7610] loss:5.2828 || sec:5.8905\n",
      "[7620] loss:5.3313 || sec:6.0190\n",
      "[7630] loss:5.1827 || sec:5.7028\n",
      "[7640] loss:4.9059 || sec:5.9374\n",
      "[7650] loss:4.9998 || sec:6.0772\n",
      "[7660] loss:5.4130 || sec:6.1827\n",
      "[7670] loss:5.1559 || sec:6.7126\n",
      "[7680] loss:5.3650 || sec:6.6185\n",
      "[7690] loss:5.3286 || sec:6.1714\n",
      "[43] train_loss: 942.5579 || val_loss: 0.0000 || sec:111.2429\n",
      "Epoch 44/50\n",
      "[train]\n",
      "[7700] loss:5.4136 || sec:1.7513\n",
      "[7710] loss:5.4578 || sec:6.2824\n",
      "[7720] loss:4.7009 || sec:6.4716\n",
      "[7730] loss:4.8802 || sec:6.3120\n",
      "[7740] loss:5.2317 || sec:6.1103\n",
      "[7750] loss:5.7331 || sec:6.3257\n",
      "[7760] loss:5.0503 || sec:6.0472\n",
      "[7770] loss:5.1177 || sec:6.0974\n",
      "[7780] loss:5.2075 || sec:6.1834\n",
      "[7790] loss:5.2412 || sec:5.9635\n",
      "[7800] loss:5.1711 || sec:6.1133\n"
     ]
    }
   ],
   "source": [
    "num_epochs=50\n",
    "train_model(net,dataloaders_dict,criterion,optimizer,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "415c659548b03e5a7ab126dcfbb4fc7162127f64c5d6abf067896d088a8939b1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorchDeepLearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
