{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ssd_model\n",
    "지금까지 구현한 모든 class를 ssd_model.py에 저장 후 load해 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ssd_model import makeDatapathList,VOCDataset,dataTransform,anno_xml2list,od_collate_fn\n",
    "\n",
    "root_path=\"./data/VOCdevkit/VOC2012/\"\n",
    "\n",
    "datapath_list=makeDatapathList(root_path)\n",
    "train_img_list,train_anno_list=datapath_list('train')\n",
    "val_img_list,val_anno_list=datapath_list('val')\n",
    "\n",
    "voc_classes=['aeroplane','bicycle','bird','boat','bottle','bus','car','cat','chair','cow','diningtable','dog','horse','motorbike','person','pottedplant','sheep','sofa','train','tvmonitor']\n",
    "color_mean=(104,117,123)\n",
    "input_size=300\n",
    "\n",
    "train_dataset=VOCDataset(train_img_list,train_anno_list,phase=\"train\",transform=dataTransform(input_size,color_mean),transform_anno=anno_xml2list(voc_classes))\n",
    "val_dataset=VOCDataset(val_img_list,val_anno_list,phase=\"val\",transform=dataTransform(input_size,color_mean),transform_anno=anno_xml2list(voc_classes))\n",
    "\n",
    "batch_size=32\n",
    "train_dataloader=data.DataLoader(train_dataset,batch_size=batch_size,shuffle=True,collate_fn=od_collate_fn)\n",
    "val_dataloader=data.DataLoader(val_dataset,batch_size=batch_size,shuffle=False,collate_fn=od_collate_fn)\n",
    "dataloaders_dict={\"train\":train_dataloader,\"val\":val_dataloader}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG 외 모듈의 초기 값으로 He를 사용\n",
    "He: ReLU를 activation function인 경우 사용\n",
    "- Kaiming He가 처음 제안해서 함수 명이 kaiming_normal_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): Conv2d(512, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): Conv2d(1024, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (2): Conv2d(512, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): Conv2d(256, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (4): Conv2d(256, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (5): Conv2d(256, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ssd_model import SSD\n",
    "\n",
    "ssd_cfg={\n",
    "    'num_classes':21, #include background class\n",
    "    'input_size':300, \n",
    "    'bbox_aspect_num':[4,6,6,6,4,4], #DBox 화면비 \n",
    "    'feature_maps':[38,19,10,5,3,1], #각 source 별 화상 크기\n",
    "    'steps':[8,16,32,64,100,300], \n",
    "    'min_sizes':[30,60,111,162,213,264], #DBox 최소 크기\n",
    "    'max_sizes':[60,111,162,213,264,315],\n",
    "    'aspect_ratios':[[2],[2,3],[2,3],[2,3],[2],[2]]\n",
    "}\n",
    "\n",
    "net=SSD(phase=\"train\",cfg=ssd_cfg)\n",
    "\n",
    "#load weight of vgg\n",
    "vgg_weights=torch.load('./weights/vgg16_reducedfc.pth')\n",
    "net.vgg.load_state_dict(vgg_weights)\n",
    "\n",
    "\n",
    "#vgg 제외한 네트워크 He 초기화\n",
    "def weights_init(m):\n",
    "    if isinstance(m,nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias,0.0)\n",
    "\n",
    "net.extras.apply(weights_init)\n",
    "net.loc.apply(weights_init)\n",
    "net.conf.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ssd_model import MultiBoxLoss\n",
    "\n",
    "criterion=MultiBoxLoss(jaccard_thresh=0.5, neg_pos=3,device=device)\n",
    "\n",
    "optimizer=optim.SGD(net.parameters(),lr=1e-3,momentum=0.9,weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(net,dataloaders_dict,criterion,optimizer,num_epochs):\n",
    "    device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "    net.to(device)\n",
    "\n",
    "    torch.backends.cudnn.benchmark=True\n",
    "\n",
    "    iteration=1\n",
    "    epoch_train_loss=0.0\n",
    "    epoch_val_loss=0.0\n",
    "    logs=[]\n",
    "\n",
    "    for epoch in range(num_epochs+1):\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        t_epoch_start=time.time()\n",
    "        t_iter_start=time.time()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        for phase in ['train','val']:\n",
    "            if phase==\"train\":\n",
    "                net.train()\n",
    "                print(\"[train]\")\n",
    "            else:\n",
    "                if (epoch+1)%10==0:\n",
    "                    net.eval()\n",
    "                    print(\"[val]\")\n",
    "                else:\n",
    "                    continue\n",
    "            \n",
    "            for images,targets in dataloaders_dict[phase]:\n",
    "                images=images.to(device)\n",
    "                targets=[target.to(device) for target in targets]\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "                with torch.set_grad_enabled(phase=='train'):\n",
    "                    outputs=net(images)\n",
    "                    loss_l,loss_c=criterion(outputs,targets)\n",
    "                    loss=loss_l+loss_c\n",
    "                    if phase==\"train\":\n",
    "                        loss.backward()\n",
    "                        \n",
    "                        nn.utils.clip_grad_value_(net.parameters(),clip_value=2.0)\n",
    "\n",
    "                        optimizer.step()\n",
    "\n",
    "                        if iteration%10==0:\n",
    "                            t_iter_finish=time.time()\n",
    "                            print(\"[%s] loss:%.4f || sec:%.4f\"%(iteration,loss.item(),t_iter_finish-t_iter_start))\n",
    "                            t_iter_start=time.time()\n",
    "                        \n",
    "                        epoch_train_loss+=loss.item()\n",
    "                        iteration+=1\n",
    "                    else:\n",
    "                        epoch_val_loss+=loss.item()\n",
    "        t_epoch_finish=time.time()\n",
    "        print(\"[%s] train_loss: %.4f || val_loss: %.4f || sec:%.4f\"%(epoch+1,epoch_train_loss,epoch_val_loss,t_epoch_finish-t_epoch_start))\n",
    "        t_epoch_start=time.time()\n",
    "\n",
    "        log_epoch={'epoch':epoch+1,'train_loss':epoch_train_loss,'val_loss':epoch_val_loss}\n",
    "        logs.append(log_epoch)\n",
    "        df=pd.DataFrame(logs)\n",
    "        df.to_csv(\"log_output.csv\")\n",
    "\n",
    "        epoch_train_loss=0.0\n",
    "        epoch_val_loss=0.0\n",
    "        if (epoch+1)%10==0:\n",
    "            torch.save(net.state_dict(),f'weights/ssd300_{str(epoch+1)}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Epoch 1/50\n",
      "[train]\n",
      "[10] loss:18.8551 || sec:10.7139\n",
      "[20] loss:15.1328 || sec:6.8665\n",
      "[30] loss:10.7758 || sec:6.7172\n",
      "[40] loss:9.3380 || sec:6.4660\n",
      "[50] loss:9.1531 || sec:6.8116\n",
      "[60] loss:8.8223 || sec:6.4990\n",
      "[70] loss:9.3177 || sec:6.7359\n",
      "[80] loss:9.5099 || sec:6.3402\n",
      "[90] loss:8.6765 || sec:7.1307\n",
      "[100] loss:10.1284 || sec:6.7207\n",
      "[110] loss:9.2552 || sec:7.0567\n",
      "[120] loss:8.8562 || sec:6.7554\n",
      "[130] loss:8.9482 || sec:6.9028\n",
      "[140] loss:9.6521 || sec:6.6987\n",
      "[150] loss:9.0618 || sec:6.7066\n",
      "[160] loss:9.7839 || sec:6.7284\n",
      "[170] loss:8.8910 || sec:6.5956\n",
      "[1] train_loss: 1915.0481 || val_loss: 0.0000 || sec:128.7687\n",
      "Epoch 2/50\n",
      "[train]\n",
      "[180] loss:9.4705 || sec:0.5080\n",
      "[190] loss:8.2160 || sec:6.0889\n",
      "[200] loss:8.8106 || sec:6.4273\n",
      "[210] loss:9.9323 || sec:6.2250\n",
      "[220] loss:11.0503 || sec:6.4944\n",
      "[230] loss:10.6639 || sec:6.5156\n",
      "[240] loss:8.4303 || sec:6.2401\n",
      "[250] loss:8.4952 || sec:6.2426\n",
      "[260] loss:9.8443 || sec:6.1211\n",
      "[270] loss:9.4811 || sec:6.3372\n",
      "[280] loss:9.4861 || sec:6.5906\n",
      "[290] loss:9.9207 || sec:6.2818\n",
      "[300] loss:9.2178 || sec:6.1742\n",
      "[310] loss:9.4009 || sec:6.2553\n",
      "[320] loss:8.7909 || sec:6.2930\n",
      "[330] loss:10.9903 || sec:6.9207\n",
      "[340] loss:9.3424 || sec:6.5642\n",
      "[350] loss:7.8627 || sec:6.5286\n",
      "[2] train_loss: 1707.9612 || val_loss: 0.0000 || sec:116.6662\n",
      "Epoch 3/50\n",
      "[train]\n",
      "[360] loss:9.4039 || sec:1.1881\n",
      "[370] loss:9.0391 || sec:6.3087\n",
      "[380] loss:8.9403 || sec:6.6209\n",
      "[390] loss:8.7850 || sec:6.2862\n",
      "[400] loss:9.3194 || sec:6.5946\n",
      "[410] loss:9.0859 || sec:6.1957\n",
      "[420] loss:10.2712 || sec:6.4598\n",
      "[430] loss:8.3048 || sec:6.2531\n",
      "[440] loss:11.0391 || sec:6.1881\n",
      "[450] loss:10.1732 || sec:6.4370\n",
      "[460] loss:8.5073 || sec:6.2099\n",
      "[470] loss:10.5565 || sec:6.0171\n",
      "[480] loss:8.1452 || sec:6.3376\n",
      "[490] loss:10.3436 || sec:6.3950\n",
      "[500] loss:9.4725 || sec:6.4043\n",
      "[510] loss:10.5062 || sec:6.1093\n",
      "[520] loss:9.5645 || sec:6.0925\n",
      "[530] loss:9.4939 || sec:6.4475\n",
      "[3] train_loss: 1718.6230 || val_loss: 0.0000 || sec:115.6016\n",
      "Epoch 4/50\n",
      "[train]\n",
      "[540] loss:9.0014 || sec:1.8763\n",
      "[550] loss:9.8681 || sec:6.2400\n",
      "[560] loss:8.8695 || sec:6.2557\n",
      "[570] loss:10.1339 || sec:6.4063\n",
      "[580] loss:10.6475 || sec:6.2164\n",
      "[590] loss:9.1410 || sec:6.4465\n",
      "[600] loss:9.9764 || sec:6.3781\n",
      "[610] loss:8.9315 || sec:6.3860\n",
      "[620] loss:9.9133 || sec:6.3680\n",
      "[630] loss:9.2224 || sec:6.3619\n",
      "[640] loss:9.2326 || sec:6.2882\n",
      "[650] loss:7.9256 || sec:6.4723\n",
      "[660] loss:9.0093 || sec:6.4144\n",
      "[670] loss:8.5237 || sec:6.2681\n",
      "[680] loss:7.6443 || sec:6.2405\n",
      "[690] loss:9.3050 || sec:6.3080\n",
      "[700] loss:9.9761 || sec:6.3218\n",
      "[710] loss:10.6123 || sec:6.0955\n",
      "[4] train_loss: 1728.9024 || val_loss: 0.0000 || sec:115.6381\n",
      "Epoch 5/50\n",
      "[train]\n",
      "[720] loss:8.4944 || sec:2.5153\n",
      "[730] loss:10.5687 || sec:6.2350\n",
      "[740] loss:9.8412 || sec:6.4797\n",
      "[750] loss:11.3064 || sec:6.2636\n",
      "[760] loss:8.1588 || sec:6.2852\n",
      "[770] loss:8.8568 || sec:6.2009\n",
      "[780] loss:9.6700 || sec:6.0653\n",
      "[790] loss:9.8219 || sec:6.2005\n",
      "[800] loss:8.1921 || sec:6.1306\n",
      "[810] loss:11.1811 || sec:6.1915\n",
      "[820] loss:10.8453 || sec:6.1501\n",
      "[830] loss:10.9412 || sec:6.1830\n",
      "[840] loss:9.4230 || sec:6.3144\n",
      "[850] loss:10.5249 || sec:6.2633\n",
      "[860] loss:9.3059 || sec:6.1858\n",
      "[870] loss:9.6800 || sec:5.9399\n",
      "[880] loss:9.3603 || sec:6.0659\n",
      "[890] loss:9.5968 || sec:6.1475\n",
      "[5] train_loss: 1741.0764 || val_loss: 0.0000 || sec:113.4464\n",
      "Epoch 6/50\n",
      "[train]\n",
      "[900] loss:10.5472 || sec:2.9301\n",
      "[910] loss:8.6272 || sec:6.2208\n",
      "[920] loss:10.5833 || sec:6.3514\n",
      "[930] loss:10.5141 || sec:6.0460\n",
      "[940] loss:10.0493 || sec:5.9998\n",
      "[950] loss:10.7183 || sec:6.2105\n",
      "[960] loss:9.4930 || sec:6.0232\n",
      "[970] loss:10.1312 || sec:6.0430\n",
      "[980] loss:8.0083 || sec:6.0642\n",
      "[990] loss:10.7566 || sec:6.3308\n",
      "[1000] loss:9.5489 || sec:6.1691\n",
      "[1010] loss:9.4520 || sec:6.1685\n",
      "[1020] loss:10.7359 || sec:6.2467\n",
      "[1030] loss:11.2286 || sec:6.4190\n",
      "[1040] loss:8.1921 || sec:6.0241\n",
      "[1050] loss:9.9638 || sec:6.2272\n",
      "[1060] loss:9.6137 || sec:6.1650\n",
      "[1070] loss:9.4955 || sec:7.0220\n",
      "[6] train_loss: 1756.0674 || val_loss: 0.0000 || sec:114.0217\n",
      "Epoch 7/50\n",
      "[train]\n",
      "[1080] loss:9.9400 || sec:4.2858\n",
      "[1090] loss:10.2165 || sec:6.3999\n",
      "[1100] loss:8.0759 || sec:6.2833\n",
      "[1110] loss:9.2589 || sec:6.3446\n",
      "[1120] loss:9.2192 || sec:6.0713\n",
      "[1130] loss:10.0214 || sec:6.2531\n",
      "[1140] loss:10.2454 || sec:6.1355\n",
      "[1150] loss:9.7189 || sec:6.0440\n",
      "[1160] loss:10.4821 || sec:6.1740\n",
      "[1170] loss:9.0736 || sec:6.3402\n",
      "[1180] loss:8.3862 || sec:5.9581\n",
      "[1190] loss:9.3709 || sec:6.0211\n",
      "[1200] loss:9.9879 || sec:6.4600\n",
      "[1210] loss:10.9218 || sec:6.3220\n",
      "[1220] loss:10.9862 || sec:6.0287\n",
      "[1230] loss:10.1812 || sec:6.1667\n",
      "[1240] loss:9.1170 || sec:6.1153\n",
      "[1250] loss:10.5697 || sec:6.2719\n",
      "[7] train_loss: 1772.9769 || val_loss: 0.0000 || sec:114.0343\n",
      "Epoch 8/50\n",
      "[train]\n",
      "[1260] loss:10.9246 || sec:4.1373\n",
      "[1270] loss:9.6917 || sec:6.0760\n",
      "[1280] loss:8.7521 || sec:6.1437\n",
      "[1290] loss:9.9740 || sec:6.0195\n",
      "[1300] loss:12.1693 || sec:6.0143\n",
      "[1310] loss:11.6372 || sec:6.1291\n",
      "[1320] loss:10.1236 || sec:6.0602\n",
      "[1330] loss:9.2142 || sec:6.2215\n",
      "[1340] loss:8.7677 || sec:6.1307\n",
      "[1350] loss:8.7595 || sec:5.8627\n",
      "[1360] loss:8.5532 || sec:6.0805\n",
      "[1370] loss:9.0011 || sec:6.2239\n",
      "[1380] loss:7.3008 || sec:6.2871\n",
      "[1390] loss:10.3677 || sec:6.1418\n",
      "[1400] loss:9.4891 || sec:6.0939\n",
      "[1410] loss:9.9329 || sec:6.1403\n",
      "[1420] loss:11.2883 || sec:6.4048\n",
      "[1430] loss:10.5346 || sec:5.9780\n",
      "[8] train_loss: 1729.0477 || val_loss: 0.0000 || sec:111.8838\n",
      "Epoch 9/50\n",
      "[train]\n",
      "[1440] loss:9.5582 || sec:4.8521\n",
      "[1450] loss:8.6084 || sec:6.0819\n",
      "[1460] loss:15.9960 || sec:6.7389\n",
      "[1470] loss:10.8292 || sec:7.2519\n",
      "[1480] loss:8.6488 || sec:6.5016\n",
      "[1490] loss:8.8256 || sec:6.4520\n",
      "[1500] loss:10.6066 || sec:6.3157\n",
      "[1510] loss:8.0315 || sec:6.8393\n",
      "[1520] loss:10.7083 || sec:6.4969\n",
      "[1530] loss:9.4184 || sec:7.0280\n",
      "[1540] loss:12.2895 || sec:6.6184\n",
      "[1550] loss:8.5876 || sec:6.3290\n",
      "[1560] loss:9.7304 || sec:6.8098\n",
      "[1570] loss:9.4570 || sec:6.5109\n",
      "[1580] loss:9.6691 || sec:6.3943\n",
      "[1590] loss:8.7768 || sec:6.5029\n",
      "[1600] loss:8.4734 || sec:6.5420\n",
      "[1610] loss:11.7507 || sec:6.2884\n",
      "[9] train_loss: 1770.1721 || val_loss: 0.0000 || sec:119.7493\n",
      "Epoch 10/50\n",
      "[train]\n",
      "[1620] loss:10.0676 || sec:5.7373\n",
      "[1630] loss:8.0152 || sec:6.6601\n",
      "[1640] loss:9.9776 || sec:6.7407\n",
      "[1650] loss:9.2585 || sec:6.3708\n",
      "[1660] loss:9.9821 || sec:6.4492\n",
      "[1670] loss:11.1475 || sec:6.2074\n",
      "[1680] loss:10.9056 || sec:6.3695\n",
      "[1690] loss:8.8696 || sec:6.3596\n",
      "[1700] loss:9.7846 || sec:6.2670\n",
      "[1710] loss:8.4884 || sec:6.4655\n",
      "[1720] loss:10.1968 || sec:6.6515\n",
      "[1730] loss:10.3569 || sec:6.2090\n",
      "[1740] loss:9.5565 || sec:6.5510\n",
      "[1750] loss:8.8377 || sec:6.7993\n",
      "[1760] loss:10.1450 || sec:6.4168\n",
      "[1770] loss:9.0992 || sec:6.6073\n",
      "[1780] loss:9.3825 || sec:6.5797\n",
      "[1790] loss:10.2553 || sec:6.8058\n",
      "[val]\n",
      "[10] train_loss: 1762.9071 || val_loss: 1568.3826 || sec:173.5475\n",
      "Epoch 11/50\n",
      "[train]\n",
      "[1800] loss:12.9348 || sec:6.9577\n",
      "[1810] loss:9.9250 || sec:6.8824\n",
      "[1820] loss:11.0007 || sec:6.5453\n",
      "[1830] loss:9.4142 || sec:6.5512\n",
      "[1840] loss:8.7956 || sec:6.6703\n",
      "[1850] loss:8.1633 || sec:6.3942\n",
      "[1860] loss:8.7776 || sec:6.5806\n",
      "[1870] loss:8.2964 || sec:6.6260\n",
      "[1880] loss:9.4411 || sec:6.5421\n",
      "[1890] loss:8.3921 || sec:6.8317\n",
      "[1900] loss:10.1141 || sec:6.5159\n",
      "[1910] loss:10.9937 || sec:6.2440\n",
      "[1920] loss:9.2136 || sec:6.5777\n",
      "[1930] loss:9.6018 || sec:6.5927\n",
      "[1940] loss:8.8194 || sec:6.7911\n",
      "[1950] loss:8.0813 || sec:6.6549\n",
      "[1960] loss:8.6113 || sec:6.0602\n",
      "[11] train_loss: 1757.4051 || val_loss: 0.0000 || sec:120.4565\n",
      "Epoch 12/50\n",
      "[train]\n",
      "[1970] loss:11.6671 || sec:0.5918\n",
      "[1980] loss:10.2665 || sec:6.5018\n",
      "[1990] loss:9.4980 || sec:6.5542\n",
      "[2000] loss:10.6900 || sec:6.7628\n",
      "[2010] loss:8.0023 || sec:6.7051\n",
      "[2020] loss:8.7942 || sec:6.7140\n",
      "[2030] loss:9.8250 || sec:6.7394\n",
      "[2040] loss:8.9749 || sec:6.4440\n",
      "[2050] loss:11.6006 || sec:6.4958\n",
      "[2060] loss:11.2144 || sec:6.2879\n",
      "[2070] loss:8.8259 || sec:6.3691\n",
      "[2080] loss:10.6819 || sec:6.6370\n",
      "[2090] loss:9.8286 || sec:6.3175\n",
      "[2100] loss:10.3248 || sec:6.5062\n",
      "[2110] loss:9.8962 || sec:6.7384\n",
      "[2120] loss:9.4192 || sec:6.7986\n",
      "[2130] loss:10.3434 || sec:6.3219\n",
      "[2140] loss:9.4143 || sec:6.4792\n",
      "[12] train_loss: 1722.1801 || val_loss: 0.0000 || sec:120.2209\n",
      "Epoch 13/50\n",
      "[train]\n",
      "[2150] loss:8.2222 || sec:1.2393\n",
      "[2160] loss:8.6214 || sec:6.7718\n",
      "[2170] loss:8.7541 || sec:6.4466\n",
      "[2180] loss:12.1107 || sec:6.4535\n",
      "[2190] loss:10.2949 || sec:6.5258\n",
      "[2200] loss:9.9137 || sec:6.5457\n",
      "[2210] loss:7.3157 || sec:6.3088\n",
      "[2220] loss:8.2601 || sec:6.3621\n",
      "[2230] loss:8.9487 || sec:6.4311\n",
      "[2240] loss:10.1456 || sec:6.3027\n",
      "[2250] loss:8.1704 || sec:6.4333\n",
      "[2260] loss:10.6775 || sec:6.5293\n",
      "[2270] loss:10.6508 || sec:6.6381\n",
      "[2280] loss:11.1979 || sec:6.4376\n",
      "[2290] loss:10.7426 || sec:6.6981\n",
      "[2300] loss:8.9679 || sec:6.5063\n",
      "[2310] loss:8.5012 || sec:6.8257\n",
      "[2320] loss:10.3739 || sec:6.4745\n",
      "[13] train_loss: 1720.7366 || val_loss: 0.0000 || sec:119.2761\n",
      "Epoch 14/50\n",
      "[train]\n",
      "[2330] loss:9.4096 || sec:1.9134\n",
      "[2340] loss:9.5440 || sec:7.0306\n",
      "[2350] loss:9.4148 || sec:6.8256\n",
      "[2360] loss:9.0282 || sec:6.9684\n",
      "[2370] loss:10.2738 || sec:7.0271\n",
      "[2380] loss:8.0068 || sec:6.5474\n",
      "[2390] loss:9.5849 || sec:6.4998\n",
      "[2400] loss:9.2302 || sec:6.8055\n",
      "[2410] loss:9.4438 || sec:6.8349\n",
      "[2420] loss:8.9080 || sec:6.6252\n",
      "[2430] loss:8.1474 || sec:6.5769\n",
      "[2440] loss:10.0909 || sec:7.1479\n",
      "[2450] loss:9.6362 || sec:6.5664\n",
      "[2460] loss:9.2033 || sec:6.6264\n",
      "[2470] loss:9.9642 || sec:6.8230\n",
      "[2480] loss:8.9312 || sec:6.6221\n",
      "[2490] loss:9.2432 || sec:6.3967\n",
      "[2500] loss:10.0521 || sec:6.9140\n",
      "[14] train_loss: 1661.4449 || val_loss: 0.0000 || sec:123.6020\n",
      "Epoch 15/50\n",
      "[train]\n",
      "[2510] loss:7.9464 || sec:2.6179\n",
      "[2520] loss:8.3070 || sec:6.7654\n",
      "[2530] loss:8.6573 || sec:6.8231\n",
      "[2540] loss:9.5134 || sec:6.7087\n",
      "[2550] loss:8.1597 || sec:6.7833\n",
      "[2560] loss:8.6569 || sec:6.7880\n",
      "[2570] loss:8.4923 || sec:6.6343\n",
      "[2580] loss:9.7825 || sec:6.7067\n",
      "[2590] loss:11.5108 || sec:6.4134\n",
      "[2600] loss:8.5056 || sec:6.7317\n",
      "[2610] loss:9.1777 || sec:6.4449\n",
      "[2620] loss:7.7787 || sec:6.3648\n",
      "[2630] loss:8.3608 || sec:6.4882\n",
      "[2640] loss:8.2689 || sec:6.5709\n",
      "[2650] loss:8.8937 || sec:6.4558\n",
      "[2660] loss:8.1589 || sec:6.9033\n",
      "[2670] loss:8.1843 || sec:6.6910\n",
      "[2680] loss:8.0528 || sec:6.5813\n",
      "[15] train_loss: 1587.4872 || val_loss: 0.0000 || sec:121.2816\n",
      "Epoch 16/50\n",
      "[train]\n",
      "[2690] loss:9.4520 || sec:3.1989\n",
      "[2700] loss:9.4538 || sec:6.9640\n",
      "[2710] loss:8.7588 || sec:6.9935\n",
      "[2720] loss:8.6048 || sec:6.7450\n",
      "[2730] loss:8.1970 || sec:6.5404\n",
      "[2740] loss:8.6737 || sec:6.4926\n",
      "[2750] loss:9.4184 || sec:6.4400\n",
      "[2760] loss:9.6121 || sec:6.5067\n",
      "[2770] loss:9.9271 || sec:6.3956\n",
      "[2780] loss:11.6504 || sec:6.6115\n",
      "[2790] loss:8.0921 || sec:6.5878\n",
      "[2800] loss:8.3085 || sec:7.0515\n",
      "[2810] loss:8.5310 || sec:6.8279\n",
      "[2820] loss:8.4969 || sec:6.6761\n",
      "[2830] loss:8.5765 || sec:6.4311\n",
      "[2840] loss:8.9736 || sec:6.5352\n",
      "[2850] loss:10.3178 || sec:6.4736\n",
      "[2860] loss:8.3788 || sec:6.4226\n",
      "[16] train_loss: 1619.6042 || val_loss: 0.0000 || sec:121.2852\n",
      "Epoch 17/50\n",
      "[train]\n",
      "[2870] loss:7.9479 || sec:3.9538\n",
      "[2880] loss:8.3946 || sec:6.4118\n",
      "[2890] loss:9.8342 || sec:6.8770\n",
      "[2900] loss:9.1891 || sec:6.9626\n",
      "[2910] loss:9.9042 || sec:6.6698\n",
      "[2920] loss:8.7746 || sec:7.0016\n",
      "[2930] loss:8.5198 || sec:6.5264\n",
      "[2940] loss:9.2511 || sec:6.8495\n",
      "[2950] loss:9.4230 || sec:7.2430\n",
      "[2960] loss:7.8185 || sec:6.5005\n",
      "[2970] loss:8.1493 || sec:7.0411\n",
      "[2980] loss:11.1118 || sec:6.7251\n",
      "[2990] loss:8.8858 || sec:6.6259\n",
      "[3000] loss:8.9854 || sec:6.9783\n",
      "[3010] loss:7.9937 || sec:6.4076\n",
      "[3020] loss:9.3319 || sec:6.8895\n",
      "[3030] loss:8.0175 || sec:6.7564\n",
      "[3040] loss:11.0043 || sec:6.6220\n",
      "[17] train_loss: 1672.7580 || val_loss: 0.0000 || sec:123.6590\n",
      "Epoch 18/50\n",
      "[train]\n",
      "[3050] loss:9.2971 || sec:4.8698\n",
      "[3060] loss:10.3588 || sec:6.6005\n",
      "[3070] loss:10.0302 || sec:6.6075\n",
      "[3080] loss:9.7265 || sec:6.7492\n",
      "[3090] loss:10.0199 || sec:6.6596\n",
      "[3100] loss:9.0752 || sec:6.6353\n",
      "[3110] loss:9.0417 || sec:6.4013\n",
      "[3120] loss:10.5027 || sec:6.8812\n",
      "[3130] loss:8.3178 || sec:6.5759\n",
      "[3140] loss:8.1458 || sec:7.1370\n",
      "[3150] loss:8.0883 || sec:6.7816\n",
      "[3160] loss:7.5121 || sec:6.9202\n",
      "[3170] loss:11.7709 || sec:6.9882\n",
      "[3180] loss:11.1363 || sec:6.6397\n",
      "[3190] loss:9.9992 || sec:6.8019\n",
      "[3200] loss:9.8916 || sec:6.5526\n",
      "[3210] loss:8.6547 || sec:6.4634\n",
      "[3220] loss:9.0265 || sec:6.6869\n",
      "[18] train_loss: 1683.8628 || val_loss: 0.0000 || sec:122.8747\n",
      "Epoch 19/50\n",
      "[train]\n",
      "[3230] loss:9.8581 || sec:5.0432\n",
      "[3240] loss:8.0235 || sec:6.4110\n",
      "[3250] loss:8.4635 || sec:6.1118\n",
      "[3260] loss:9.3394 || sec:6.4089\n",
      "[3270] loss:9.7781 || sec:6.4306\n",
      "[3280] loss:9.1753 || sec:6.2618\n",
      "[3290] loss:9.2393 || sec:6.5860\n",
      "[3300] loss:8.5018 || sec:6.5261\n",
      "[3310] loss:8.7263 || sec:6.3634\n",
      "[3320] loss:8.4858 || sec:6.5491\n",
      "[3330] loss:8.9690 || sec:6.7444\n",
      "[3340] loss:8.5236 || sec:6.3216\n",
      "[3350] loss:9.3320 || sec:6.6676\n",
      "[3360] loss:8.1911 || sec:6.6476\n",
      "[3370] loss:8.5503 || sec:6.6270\n",
      "[3380] loss:8.3902 || sec:6.6074\n",
      "[3390] loss:9.1112 || sec:6.7803\n",
      "[3400] loss:8.5604 || sec:6.5244\n",
      "[19] train_loss: 1601.8438 || val_loss: 0.0000 || sec:118.9324\n",
      "Epoch 20/50\n",
      "[train]\n",
      "[3410] loss:8.1520 || sec:5.9758\n",
      "[3420] loss:8.6442 || sec:6.4672\n",
      "[3430] loss:8.2179 || sec:6.9106\n",
      "[3440] loss:9.8983 || sec:6.5798\n",
      "[3450] loss:9.6126 || sec:6.5616\n",
      "[3460] loss:9.1978 || sec:6.7456\n",
      "[3470] loss:8.5979 || sec:6.2978\n",
      "[3480] loss:9.4704 || sec:6.3147\n",
      "[3490] loss:8.2757 || sec:6.4508\n",
      "[3500] loss:8.5082 || sec:6.3606\n",
      "[3510] loss:8.3370 || sec:6.6449\n",
      "[3520] loss:8.1117 || sec:6.6865\n",
      "[3530] loss:8.2475 || sec:6.6260\n",
      "[3540] loss:9.5406 || sec:6.4502\n",
      "[3550] loss:8.6461 || sec:6.6347\n",
      "[3560] loss:8.5601 || sec:7.0651\n",
      "[3570] loss:9.7272 || sec:6.6944\n",
      "[3580] loss:7.3152 || sec:6.4643\n",
      "[val]\n",
      "[20] train_loss: 1548.1110 || val_loss: 1459.4287 || sec:170.7595\n",
      "Epoch 21/50\n",
      "[train]\n",
      "[3590] loss:9.6629 || sec:6.6523\n",
      "[3600] loss:11.7760 || sec:6.6841\n",
      "[3610] loss:9.5494 || sec:6.6500\n",
      "[3620] loss:10.2167 || sec:6.7499\n",
      "[3630] loss:8.7451 || sec:6.6153\n",
      "[3640] loss:9.7684 || sec:6.2961\n",
      "[3650] loss:8.7687 || sec:6.8024\n",
      "[3660] loss:8.8067 || sec:6.9997\n",
      "[3670] loss:9.6136 || sec:6.7464\n",
      "[3680] loss:10.3660 || sec:6.4377\n",
      "[3690] loss:8.6472 || sec:6.5291\n",
      "[3700] loss:8.4339 || sec:6.6289\n",
      "[3710] loss:7.4433 || sec:6.4867\n",
      "[3720] loss:8.8895 || sec:6.3132\n",
      "[3730] loss:7.7799 || sec:6.3367\n",
      "[3740] loss:8.2464 || sec:6.2192\n",
      "[3750] loss:9.7372 || sec:6.4897\n",
      "[21] train_loss: 1621.3182 || val_loss: 0.0000 || sec:119.9157\n",
      "Epoch 22/50\n",
      "[train]\n",
      "[3760] loss:9.6805 || sec:0.5577\n",
      "[3770] loss:8.3877 || sec:6.5288\n",
      "[3780] loss:8.2192 || sec:6.5134\n",
      "[3790] loss:9.6537 || sec:5.9671\n",
      "[3800] loss:9.2526 || sec:6.4789\n",
      "[3810] loss:9.8495 || sec:6.4301\n",
      "[3820] loss:9.5413 || sec:6.6800\n",
      "[3830] loss:9.4345 || sec:6.2517\n",
      "[3840] loss:7.6654 || sec:6.2200\n",
      "[3850] loss:9.2539 || sec:6.3852\n",
      "[3860] loss:7.6657 || sec:6.4415\n",
      "[3870] loss:8.9347 || sec:6.4605\n",
      "[3880] loss:8.8370 || sec:6.3378\n",
      "[3890] loss:8.9363 || sec:6.2833\n",
      "[3900] loss:7.6588 || sec:6.4070\n",
      "[3910] loss:8.9758 || sec:6.2260\n",
      "[3920] loss:8.0313 || sec:6.1652\n",
      "[3930] loss:8.4153 || sec:6.1620\n",
      "[22] train_loss: 1619.3951 || val_loss: 0.0000 || sec:116.2817\n",
      "Epoch 23/50\n",
      "[train]\n",
      "[3940] loss:8.5427 || sec:1.1649\n",
      "[3950] loss:8.3002 || sec:6.2324\n",
      "[3960] loss:9.5619 || sec:6.4087\n",
      "[3970] loss:8.0410 || sec:6.4339\n",
      "[3980] loss:8.1857 || sec:6.2622\n",
      "[3990] loss:8.5706 || sec:6.2517\n",
      "[4000] loss:8.6363 || sec:6.4052\n",
      "[4010] loss:9.1440 || sec:6.4628\n",
      "[4020] loss:7.5550 || sec:6.3079\n",
      "[4030] loss:8.8925 || sec:6.3052\n",
      "[4040] loss:8.5127 || sec:6.3147\n",
      "[4050] loss:9.8385 || sec:6.3583\n",
      "[4060] loss:7.6592 || sec:6.5277\n",
      "[4070] loss:10.2604 || sec:6.3852\n",
      "[4080] loss:8.3316 || sec:6.2957\n",
      "[4090] loss:8.7098 || sec:6.2288\n",
      "[4100] loss:9.4580 || sec:6.2744\n",
      "[4110] loss:10.0413 || sec:6.3815\n",
      "[23] train_loss: 1585.3400 || val_loss: 0.0000 || sec:116.1202\n",
      "Epoch 24/50\n",
      "[train]\n",
      "[4120] loss:9.5704 || sec:1.7233\n",
      "[4130] loss:8.8814 || sec:6.5035\n",
      "[4140] loss:9.2930 || sec:6.4028\n",
      "[4150] loss:8.4086 || sec:6.4313\n",
      "[4160] loss:8.7065 || sec:6.3870\n",
      "[4170] loss:8.5467 || sec:6.4372\n",
      "[4180] loss:8.7217 || sec:6.5836\n",
      "[4190] loss:8.6974 || sec:6.4699\n",
      "[4200] loss:9.0437 || sec:6.2403\n",
      "[4210] loss:11.2953 || sec:6.4866\n",
      "[4220] loss:9.4938 || sec:6.1414\n",
      "[4230] loss:8.9847 || sec:6.3734\n",
      "[4240] loss:8.2647 || sec:6.4673\n",
      "[4250] loss:8.2558 || sec:6.2889\n",
      "[4260] loss:7.6665 || sec:6.4209\n",
      "[4270] loss:8.6452 || sec:6.5537\n",
      "[4280] loss:9.7236 || sec:6.2734\n",
      "[4290] loss:8.9396 || sec:6.5294\n",
      "[24] train_loss: 1576.7693 || val_loss: 0.0000 || sec:117.1112\n",
      "Epoch 25/50\n",
      "[train]\n",
      "[4300] loss:9.0469 || sec:2.4742\n",
      "[4310] loss:7.7439 || sec:6.5031\n",
      "[4320] loss:7.8308 || sec:6.3232\n",
      "[4330] loss:9.7859 || sec:6.3474\n",
      "[4340] loss:9.2118 || sec:6.3727\n",
      "[4350] loss:8.4453 || sec:7.1764\n",
      "[4360] loss:7.9968 || sec:6.9443\n",
      "[4370] loss:11.4854 || sec:7.4110\n",
      "[4380] loss:8.3166 || sec:7.1163\n",
      "[4390] loss:8.8566 || sec:6.3386\n",
      "[4400] loss:8.4728 || sec:6.6268\n",
      "[4410] loss:9.1016 || sec:6.4250\n",
      "[4420] loss:9.6624 || sec:6.6786\n",
      "[4430] loss:8.8206 || sec:6.2666\n",
      "[4440] loss:9.3429 || sec:6.4276\n",
      "[4450] loss:8.1436 || sec:6.3810\n",
      "[4460] loss:8.2417 || sec:6.3878\n",
      "[4470] loss:8.7913 || sec:6.4156\n",
      "[25] train_loss: 1597.7855 || val_loss: 0.0000 || sec:120.5617\n",
      "Epoch 26/50\n",
      "[train]\n",
      "[4480] loss:8.1371 || sec:3.2249\n",
      "[4490] loss:9.1069 || sec:6.4617\n",
      "[4500] loss:8.9784 || sec:6.4445\n",
      "[4510] loss:9.1694 || sec:6.5318\n",
      "[4520] loss:7.6619 || sec:6.5836\n",
      "[4530] loss:8.5335 || sec:6.3600\n",
      "[4540] loss:9.2497 || sec:6.6372\n",
      "[4550] loss:8.9625 || sec:6.6294\n",
      "[4560] loss:10.2418 || sec:6.4913\n",
      "[4570] loss:9.4587 || sec:6.3482\n",
      "[4580] loss:9.4942 || sec:6.3651\n",
      "[4590] loss:9.6933 || sec:6.6189\n",
      "[4600] loss:8.5768 || sec:6.5268\n",
      "[4610] loss:8.8166 || sec:6.6712\n",
      "[4620] loss:9.6573 || sec:6.4394\n",
      "[4630] loss:7.5099 || sec:6.3631\n",
      "[4640] loss:8.9375 || sec:6.3950\n",
      "[4650] loss:8.1342 || sec:6.5934\n",
      "[26] train_loss: 1557.7133 || val_loss: 0.0000 || sec:118.8905\n",
      "Epoch 27/50\n",
      "[train]\n",
      "[4660] loss:8.1839 || sec:3.6306\n",
      "[4670] loss:8.4264 || sec:6.5246\n",
      "[4680] loss:8.3780 || sec:6.4066\n",
      "[4690] loss:7.8066 || sec:6.3097\n",
      "[4700] loss:7.5528 || sec:6.2146\n",
      "[4710] loss:7.8882 || sec:6.0670\n",
      "[4720] loss:8.5009 || sec:6.4892\n",
      "[4730] loss:8.3955 || sec:6.5749\n",
      "[4740] loss:7.7575 || sec:6.5702\n",
      "[4750] loss:9.6532 || sec:6.6090\n",
      "[4760] loss:9.0766 || sec:6.6156\n",
      "[4770] loss:8.8185 || sec:6.5529\n",
      "[4780] loss:8.3004 || sec:6.7432\n",
      "[4790] loss:9.6207 || sec:6.5680\n",
      "[4800] loss:8.6646 || sec:6.6075\n",
      "[4810] loss:8.2330 || sec:6.5504\n",
      "[4820] loss:7.9345 || sec:6.7135\n",
      "[4830] loss:8.6908 || sec:6.5009\n",
      "[27] train_loss: 1521.8214 || val_loss: 0.0000 || sec:118.8624\n",
      "Epoch 28/50\n",
      "[train]\n",
      "[4840] loss:8.6136 || sec:4.6707\n",
      "[4850] loss:8.7932 || sec:6.5936\n",
      "[4860] loss:9.0871 || sec:6.6315\n",
      "[4870] loss:8.5612 || sec:6.4104\n",
      "[4880] loss:8.8591 || sec:6.3860\n",
      "[4890] loss:10.0418 || sec:6.5515\n",
      "[4900] loss:7.8980 || sec:6.3842\n",
      "[4910] loss:8.0041 || sec:6.4228\n",
      "[4920] loss:8.5121 || sec:6.3155\n",
      "[4930] loss:7.9847 || sec:6.4176\n",
      "[4940] loss:8.6817 || sec:6.5641\n",
      "[4950] loss:8.3540 || sec:6.4555\n",
      "[4960] loss:9.2777 || sec:6.4290\n",
      "[4970] loss:7.9718 || sec:6.5065\n",
      "[4980] loss:8.3419 || sec:6.4747\n",
      "[4990] loss:7.9343 || sec:6.4255\n",
      "[5000] loss:8.0688 || sec:6.4908\n",
      "[5010] loss:8.8716 || sec:6.5148\n",
      "[28] train_loss: 1507.2147 || val_loss: 0.0000 || sec:118.4165\n",
      "Epoch 29/50\n",
      "[train]\n",
      "[5020] loss:8.1046 || sec:4.9010\n",
      "[5030] loss:7.9611 || sec:6.4894\n",
      "[5040] loss:7.9682 || sec:6.4718\n",
      "[5050] loss:8.3772 || sec:6.3427\n",
      "[5060] loss:8.4993 || sec:6.5284\n",
      "[5070] loss:9.9803 || sec:6.5321\n",
      "[5080] loss:7.9273 || sec:6.2453\n",
      "[5090] loss:8.0916 || sec:6.1660\n",
      "[5100] loss:8.2136 || sec:6.6154\n",
      "[5110] loss:7.8627 || sec:6.4504\n",
      "[5120] loss:7.6286 || sec:6.5083\n",
      "[5130] loss:7.5996 || sec:6.4621\n",
      "[5140] loss:8.4089 || sec:6.4746\n",
      "[5150] loss:8.2302 || sec:6.5343\n",
      "[5160] loss:8.2324 || sec:6.2505\n",
      "[5170] loss:7.7858 || sec:6.3943\n",
      "[5180] loss:7.9014 || sec:6.7604\n",
      "[5190] loss:7.8236 || sec:6.3409\n",
      "[29] train_loss: 1438.6690 || val_loss: 0.0000 || sec:117.6699\n",
      "Epoch 30/50\n",
      "[train]\n",
      "[5200] loss:8.1474 || sec:5.7529\n",
      "[5210] loss:7.6801 || sec:6.4055\n",
      "[5220] loss:7.8266 || sec:6.3664\n",
      "[5230] loss:7.7589 || sec:6.3419\n",
      "[5240] loss:7.9597 || sec:6.6157\n",
      "[5250] loss:7.7804 || sec:6.1959\n",
      "[5260] loss:8.3175 || sec:6.3503\n",
      "[5270] loss:7.8714 || sec:6.6443\n",
      "[5280] loss:7.8817 || sec:6.3480\n",
      "[5290] loss:7.8500 || sec:6.4495\n",
      "[5300] loss:8.2111 || sec:6.5222\n",
      "[5310] loss:7.9482 || sec:6.3598\n",
      "[5320] loss:7.5809 || sec:6.6061\n",
      "[5330] loss:8.5737 || sec:6.7165\n",
      "[5340] loss:7.4966 || sec:6.4138\n",
      "[5350] loss:7.4171 || sec:6.4560\n",
      "[5360] loss:9.0124 || sec:6.1013\n",
      "[5370] loss:7.8872 || sec:6.4080\n",
      "[val]\n",
      "[30] train_loss: 1406.6111 || val_loss: 1394.3706 || sec:166.9507\n",
      "Epoch 31/50\n",
      "[train]\n",
      "[5380] loss:7.9147 || sec:6.4196\n",
      "[5390] loss:8.1905 || sec:6.4955\n",
      "[5400] loss:8.1552 || sec:6.4184\n",
      "[5410] loss:7.5818 || sec:6.3620\n",
      "[5420] loss:7.8845 || sec:6.5385\n",
      "[5430] loss:7.9588 || sec:6.3116\n",
      "[5440] loss:7.7577 || sec:6.5409\n",
      "[5450] loss:9.1884 || sec:6.4337\n",
      "[5460] loss:7.4951 || sec:6.3593\n",
      "[5470] loss:7.4640 || sec:6.2168\n",
      "[5480] loss:7.5882 || sec:6.4571\n",
      "[5490] loss:7.9291 || sec:6.3864\n",
      "[5500] loss:7.9404 || sec:6.1180\n",
      "[5510] loss:7.4419 || sec:6.5237\n",
      "[5520] loss:8.2252 || sec:6.5518\n",
      "[5530] loss:8.0880 || sec:6.7510\n",
      "[5540] loss:7.3781 || sec:6.2421\n",
      "[31] train_loss: 1403.1158 || val_loss: 0.0000 || sec:117.6475\n",
      "Epoch 32/50\n",
      "[train]\n",
      "[5550] loss:7.8220 || sec:0.4493\n",
      "[5560] loss:8.1881 || sec:6.5478\n",
      "[5570] loss:7.4965 || sec:6.7103\n",
      "[5580] loss:7.5081 || sec:6.4166\n",
      "[5590] loss:8.2617 || sec:6.3719\n",
      "[5600] loss:7.6889 || sec:6.7282\n",
      "[5610] loss:8.4077 || sec:6.6691\n",
      "[5620] loss:7.1700 || sec:6.3541\n",
      "[5630] loss:7.9466 || sec:6.4205\n",
      "[5640] loss:7.4847 || sec:6.5660\n",
      "[5650] loss:7.6804 || sec:6.3717\n",
      "[5660] loss:7.5856 || sec:6.3550\n",
      "[5670] loss:7.7322 || sec:6.5760\n",
      "[5680] loss:7.9081 || sec:6.4897\n",
      "[5690] loss:7.5571 || sec:6.2814\n",
      "[5700] loss:7.7359 || sec:6.3975\n",
      "[5710] loss:7.8793 || sec:6.3021\n",
      "[5720] loss:7.6086 || sec:6.2696\n",
      "[32] train_loss: 1399.9794 || val_loss: 0.0000 || sec:118.2936\n",
      "Epoch 33/50\n",
      "[train]\n",
      "[5730] loss:7.5140 || sec:1.1238\n",
      "[5740] loss:7.7487 || sec:6.5059\n",
      "[5750] loss:7.4197 || sec:6.6188\n",
      "[5760] loss:7.7457 || sec:6.5518\n",
      "[5770] loss:8.1995 || sec:6.5393\n",
      "[5780] loss:8.9852 || sec:6.6111\n",
      "[5790] loss:8.0975 || sec:6.5370\n",
      "[5800] loss:7.9658 || sec:6.5697\n",
      "[5810] loss:7.8286 || sec:6.4500\n",
      "[5820] loss:7.5564 || sec:6.4382\n",
      "[5830] loss:7.2147 || sec:6.4606\n",
      "[5840] loss:7.5916 || sec:6.2726\n",
      "[5850] loss:7.2207 || sec:6.5533\n",
      "[5860] loss:7.3890 || sec:6.2989\n",
      "[5870] loss:8.1723 || sec:6.4774\n",
      "[5880] loss:7.6630 || sec:6.4218\n",
      "[5890] loss:8.0148 || sec:6.3416\n",
      "[5900] loss:7.7350 || sec:6.4402\n",
      "[33] train_loss: 1416.6590 || val_loss: 0.0000 || sec:118.4439\n",
      "Epoch 34/50\n",
      "[train]\n",
      "[5910] loss:7.5419 || sec:1.8082\n",
      "[5920] loss:7.5444 || sec:6.3146\n",
      "[5930] loss:7.5120 || sec:6.4761\n",
      "[5940] loss:8.4339 || sec:6.2849\n",
      "[5950] loss:7.6662 || sec:6.5868\n",
      "[5960] loss:8.5785 || sec:6.4001\n",
      "[5970] loss:7.7841 || sec:6.3172\n",
      "[5980] loss:7.7154 || sec:6.3607\n",
      "[5990] loss:7.6949 || sec:6.5901\n",
      "[6000] loss:7.2787 || sec:6.5136\n",
      "[6010] loss:7.8915 || sec:6.1491\n",
      "[6020] loss:8.0918 || sec:6.5223\n",
      "[6030] loss:7.9305 || sec:6.0468\n",
      "[6040] loss:7.5571 || sec:6.7338\n",
      "[6050] loss:7.9302 || sec:6.8250\n",
      "[6060] loss:7.3842 || sec:6.4565\n",
      "[6070] loss:8.5317 || sec:6.3451\n",
      "[6080] loss:8.6357 || sec:6.4995\n",
      "[34] train_loss: 1396.4856 || val_loss: 0.0000 || sec:117.7065\n",
      "Epoch 35/50\n",
      "[train]\n",
      "[6090] loss:8.5980 || sec:2.4718\n",
      "[6100] loss:7.3336 || sec:6.6271\n",
      "[6110] loss:7.7391 || sec:6.5436\n",
      "[6120] loss:7.6195 || sec:6.3415\n",
      "[6130] loss:7.5649 || sec:6.4421\n",
      "[6140] loss:7.7365 || sec:6.4536\n",
      "[6150] loss:8.0157 || sec:6.5663\n",
      "[6160] loss:7.4410 || sec:6.7877\n",
      "[6170] loss:7.7770 || sec:6.6527\n",
      "[6180] loss:7.9437 || sec:6.4775\n",
      "[6190] loss:7.3977 || sec:6.2505\n",
      "[6200] loss:7.3749 || sec:6.3782\n",
      "[6210] loss:8.1175 || sec:6.4686\n",
      "[6220] loss:7.2324 || sec:6.3974\n",
      "[6230] loss:7.5516 || sec:6.2106\n",
      "[6240] loss:7.9056 || sec:6.0561\n",
      "[6250] loss:8.1800 || sec:6.2755\n",
      "[6260] loss:7.6183 || sec:6.3672\n",
      "[35] train_loss: 1386.4330 || val_loss: 0.0000 || sec:117.7435\n",
      "Epoch 36/50\n",
      "[train]\n",
      "[6270] loss:7.8019 || sec:3.0712\n",
      "[6280] loss:7.5991 || sec:6.4233\n",
      "[6290] loss:7.7514 || sec:6.4048\n",
      "[6300] loss:8.1431 || sec:6.2937\n",
      "[6310] loss:9.3456 || sec:6.7638\n",
      "[6320] loss:8.2322 || sec:6.5072\n",
      "[6330] loss:7.7121 || sec:6.1791\n",
      "[6340] loss:7.6081 || sec:6.2926\n",
      "[6350] loss:7.5383 || sec:6.5472\n",
      "[6360] loss:7.5712 || sec:6.6104\n",
      "[6370] loss:8.0816 || sec:6.5435\n",
      "[6380] loss:8.1510 || sec:6.4232\n",
      "[6390] loss:7.7915 || sec:6.4605\n",
      "[6400] loss:7.6283 || sec:6.4956\n",
      "[6410] loss:8.5803 || sec:6.4142\n",
      "[6420] loss:7.9685 || sec:6.3583\n",
      "[6430] loss:7.7122 || sec:6.3561\n",
      "[6440] loss:7.7670 || sec:6.5885\n",
      "[36] train_loss: 1383.2235 || val_loss: 0.0000 || sec:117.9306\n",
      "Epoch 37/50\n",
      "[train]\n",
      "[6450] loss:7.7377 || sec:3.8531\n",
      "[6460] loss:7.9962 || sec:6.5901\n",
      "[6470] loss:7.8439 || sec:6.4523\n",
      "[6480] loss:7.5416 || sec:6.4215\n",
      "[6490] loss:7.8001 || sec:6.5008\n",
      "[6500] loss:7.3201 || sec:6.4604\n",
      "[6510] loss:7.6711 || sec:6.4479\n",
      "[6520] loss:8.5241 || sec:6.2785\n",
      "[6530] loss:7.8758 || sec:6.3206\n",
      "[6540] loss:7.7679 || sec:6.3275\n",
      "[6550] loss:7.5864 || sec:6.3419\n",
      "[6560] loss:7.6363 || sec:6.5391\n",
      "[6570] loss:7.5785 || sec:6.5147\n",
      "[6580] loss:7.6244 || sec:6.5113\n",
      "[6590] loss:7.8094 || sec:6.5450\n",
      "[6600] loss:7.5157 || sec:6.6533\n",
      "[6610] loss:8.2109 || sec:6.4886\n",
      "[6620] loss:8.3639 || sec:6.4400\n",
      "[37] train_loss: 1396.5618 || val_loss: 0.0000 || sec:118.2875\n",
      "Epoch 38/50\n",
      "[train]\n",
      "[6630] loss:7.9039 || sec:4.4333\n",
      "[6640] loss:7.6650 || sec:6.4850\n",
      "[6650] loss:8.4397 || sec:6.3798\n",
      "[6660] loss:7.7003 || sec:6.6051\n",
      "[6670] loss:8.1695 || sec:6.3891\n",
      "[6680] loss:8.3277 || sec:6.3091\n",
      "[6690] loss:7.5725 || sec:6.2299\n",
      "[6700] loss:7.4773 || sec:6.3411\n",
      "[6710] loss:8.5781 || sec:6.3209\n",
      "[6720] loss:8.3969 || sec:6.3677\n",
      "[6730] loss:7.7895 || sec:6.4772\n",
      "[6740] loss:7.8305 || sec:6.5964\n",
      "[6750] loss:7.6615 || sec:6.2645\n",
      "[6760] loss:7.6693 || sec:6.2728\n",
      "[6770] loss:7.8493 || sec:6.6798\n",
      "[6780] loss:8.3303 || sec:6.4343\n",
      "[6790] loss:8.4843 || sec:6.8578\n",
      "[6800] loss:7.7447 || sec:6.5895\n",
      "[38] train_loss: 1403.0305 || val_loss: 0.0000 || sec:117.9531\n",
      "Epoch 39/50\n",
      "[train]\n",
      "[6810] loss:8.2065 || sec:5.2003\n",
      "[6820] loss:8.4541 || sec:6.1941\n",
      "[6830] loss:7.4768 || sec:6.7946\n",
      "[6840] loss:7.4744 || sec:6.2797\n",
      "[6850] loss:7.3037 || sec:6.8887\n",
      "[6860] loss:8.2758 || sec:6.2882\n",
      "[6870] loss:7.8904 || sec:6.5894\n",
      "[6880] loss:7.3019 || sec:6.5219\n",
      "[6890] loss:7.8270 || sec:6.6117\n",
      "[6900] loss:7.5755 || sec:6.2539\n",
      "[6910] loss:7.8771 || sec:6.3277\n",
      "[6920] loss:7.8118 || sec:6.4985\n",
      "[6930] loss:7.9720 || sec:6.4239\n",
      "[6940] loss:7.4847 || sec:6.4286\n",
      "[6950] loss:7.5197 || sec:6.3238\n",
      "[6960] loss:7.5514 || sec:6.3072\n",
      "[6970] loss:7.6443 || sec:6.7389\n",
      "[6980] loss:7.4907 || sec:6.3281\n",
      "[39] train_loss: 1381.0539 || val_loss: 0.0000 || sec:118.2458\n",
      "Epoch 40/50\n",
      "[train]\n",
      "[6990] loss:8.7538 || sec:5.7332\n",
      "[7000] loss:8.0749 || sec:6.3265\n",
      "[7010] loss:7.4310 || sec:6.3745\n",
      "[7020] loss:7.8005 || sec:6.3882\n",
      "[7030] loss:8.3128 || sec:6.6594\n",
      "[7040] loss:7.8168 || sec:6.4466\n",
      "[7050] loss:7.5950 || sec:6.5931\n",
      "[7060] loss:7.7592 || sec:6.4803\n",
      "[7070] loss:8.0793 || sec:6.5585\n",
      "[7080] loss:7.8261 || sec:6.3253\n",
      "[7090] loss:7.1770 || sec:6.4980\n",
      "[7100] loss:8.1233 || sec:6.4056\n",
      "[7110] loss:7.8138 || sec:6.3973\n",
      "[7120] loss:7.5992 || sec:6.4913\n",
      "[7130] loss:6.8103 || sec:6.4148\n",
      "[7140] loss:8.0142 || sec:6.3401\n",
      "[7150] loss:7.6030 || sec:6.5122\n",
      "[7160] loss:7.2702 || sec:6.2799\n",
      "[val]\n",
      "[40] train_loss: 1405.5356 || val_loss: 1371.5237 || sec:167.5429\n",
      "Epoch 41/50\n",
      "[train]\n",
      "[7170] loss:7.9401 || sec:6.4655\n",
      "[7180] loss:8.3697 || sec:6.5367\n",
      "[7190] loss:7.2847 || sec:6.3568\n",
      "[7200] loss:7.5163 || sec:6.4023\n",
      "[7210] loss:7.3922 || sec:6.3372\n",
      "[7220] loss:7.4104 || sec:6.2565\n",
      "[7230] loss:6.9071 || sec:6.4289\n",
      "[7240] loss:8.0648 || sec:6.4887\n",
      "[7250] loss:7.9425 || sec:6.3142\n",
      "[7260] loss:7.7299 || sec:6.4073\n",
      "[7270] loss:8.0992 || sec:6.3113\n",
      "[7280] loss:7.8867 || sec:6.3784\n",
      "[7290] loss:7.0507 || sec:6.0645\n",
      "[7300] loss:8.2841 || sec:6.4566\n",
      "[7310] loss:7.7386 || sec:6.2468\n",
      "[7320] loss:8.0683 || sec:6.4733\n",
      "[7330] loss:7.4180 || sec:6.4709\n",
      "[41] train_loss: 1397.8325 || val_loss: 0.0000 || sec:116.9768\n",
      "Epoch 42/50\n",
      "[train]\n",
      "[7340] loss:8.1615 || sec:0.5181\n",
      "[7350] loss:8.5298 || sec:6.4727\n",
      "[7360] loss:7.5546 || sec:6.0176\n",
      "[7370] loss:7.1979 || sec:6.4228\n",
      "[7380] loss:7.7660 || sec:6.4629\n",
      "[7390] loss:7.2651 || sec:6.5488\n",
      "[7400] loss:8.1926 || sec:6.5141\n",
      "[7410] loss:7.6941 || sec:6.2530\n",
      "[7420] loss:7.2324 || sec:6.5299\n",
      "[7430] loss:7.3713 || sec:6.3455\n",
      "[7440] loss:7.8136 || sec:6.5605\n",
      "[7450] loss:7.3392 || sec:6.6609\n",
      "[7460] loss:7.5624 || sec:6.4871\n",
      "[7470] loss:8.1203 || sec:6.3908\n",
      "[7480] loss:7.3795 || sec:6.3086\n",
      "[7490] loss:7.8471 || sec:6.3598\n",
      "[7500] loss:7.3448 || sec:6.4644\n",
      "[7510] loss:8.1375 || sec:6.1899\n",
      "[42] train_loss: 1393.7420 || val_loss: 0.0000 || sec:117.2023\n",
      "Epoch 43/50\n",
      "[train]\n",
      "[7520] loss:7.9595 || sec:1.1434\n",
      "[7530] loss:7.5648 || sec:6.4023\n",
      "[7540] loss:7.8274 || sec:6.3978\n",
      "[7550] loss:7.7495 || sec:6.4345\n",
      "[7560] loss:7.7562 || sec:6.3978\n",
      "[7570] loss:8.1058 || sec:6.3809\n",
      "[7580] loss:7.3478 || sec:6.3188\n",
      "[7590] loss:7.8327 || sec:6.5362\n",
      "[7600] loss:7.5286 || sec:6.3302\n",
      "[7610] loss:7.0573 || sec:6.5398\n",
      "[7620] loss:7.7226 || sec:6.5014\n",
      "[7630] loss:7.1197 || sec:6.4828\n",
      "[7640] loss:7.3749 || sec:6.5572\n",
      "[7650] loss:7.7765 || sec:6.5545\n",
      "[7660] loss:7.3160 || sec:6.4574\n",
      "[7670] loss:8.3698 || sec:6.3965\n",
      "[7680] loss:7.9717 || sec:6.5285\n",
      "[7690] loss:7.7197 || sec:6.5019\n",
      "[43] train_loss: 1378.6231 || val_loss: 0.0000 || sec:118.1227\n",
      "Epoch 44/50\n",
      "[train]\n",
      "[7700] loss:8.2081 || sec:1.7758\n",
      "[7710] loss:8.1357 || sec:6.2681\n",
      "[7720] loss:7.8562 || sec:6.5135\n",
      "[7730] loss:7.7952 || sec:6.4118\n",
      "[7740] loss:7.5452 || sec:6.3856\n",
      "[7750] loss:8.2521 || sec:6.8336\n",
      "[7760] loss:7.5940 || sec:6.6038\n",
      "[7770] loss:7.7763 || sec:6.3375\n",
      "[7780] loss:7.7405 || sec:6.4546\n",
      "[7790] loss:7.5428 || sec:6.3408\n",
      "[7800] loss:7.6734 || sec:6.7169\n",
      "[7810] loss:7.3117 || sec:6.3290\n",
      "[7820] loss:8.3304 || sec:6.3691\n",
      "[7830] loss:7.1507 || sec:6.5520\n",
      "[7840] loss:8.0391 || sec:6.4939\n",
      "[7850] loss:7.5966 || sec:6.2223\n",
      "[7860] loss:7.3770 || sec:6.4487\n",
      "[7870] loss:8.1528 || sec:6.4331\n",
      "[44] train_loss: 1386.5345 || val_loss: 0.0000 || sec:117.9494\n",
      "Epoch 45/50\n",
      "[train]\n",
      "[7880] loss:7.9342 || sec:2.6783\n",
      "[7890] loss:8.0038 || sec:6.4273\n",
      "[7900] loss:7.6131 || sec:6.5606\n",
      "[7910] loss:7.1487 || sec:6.4084\n",
      "[7920] loss:7.7818 || sec:6.4778\n",
      "[7930] loss:7.5724 || sec:6.5479\n",
      "[7940] loss:7.5278 || sec:6.3466\n",
      "[7950] loss:7.7834 || sec:6.4903\n",
      "[7960] loss:8.2634 || sec:6.8656\n",
      "[7970] loss:7.6319 || sec:6.5604\n",
      "[7980] loss:7.8411 || sec:6.2940\n",
      "[7990] loss:7.5632 || sec:6.3182\n",
      "[8000] loss:7.4408 || sec:6.2932\n",
      "[8010] loss:8.2516 || sec:6.6131\n",
      "[8020] loss:7.5922 || sec:6.3699\n",
      "[8030] loss:7.6574 || sec:6.5039\n",
      "[8040] loss:7.2754 || sec:6.2766\n",
      "[8050] loss:7.7742 || sec:6.1188\n",
      "[45] train_loss: 1365.2369 || val_loss: 0.0000 || sec:118.2047\n",
      "Epoch 46/50\n",
      "[train]\n",
      "[8060] loss:7.3874 || sec:3.2863\n",
      "[8070] loss:8.0101 || sec:6.6283\n",
      "[8080] loss:7.4172 || sec:6.3236\n",
      "[8090] loss:7.1400 || sec:6.5779\n",
      "[8100] loss:7.8012 || sec:6.5451\n",
      "[8110] loss:7.8882 || sec:6.3400\n",
      "[8120] loss:7.1263 || sec:6.3500\n",
      "[8130] loss:7.7614 || sec:6.5678\n",
      "[8140] loss:7.7002 || sec:6.5819\n",
      "[8150] loss:7.7163 || sec:6.3969\n",
      "[8160] loss:7.6605 || sec:6.4681\n",
      "[8170] loss:7.3593 || sec:6.1312\n",
      "[8180] loss:7.5835 || sec:6.3715\n",
      "[8190] loss:6.9943 || sec:6.5066\n",
      "[8200] loss:7.7190 || sec:6.5548\n",
      "[8210] loss:7.6101 || sec:6.3863\n",
      "[8220] loss:8.0948 || sec:6.6339\n",
      "[8230] loss:7.4922 || sec:6.2904\n",
      "[46] train_loss: 1360.6540 || val_loss: 0.0000 || sec:118.0125\n",
      "Epoch 47/50\n",
      "[train]\n",
      "[8240] loss:8.0701 || sec:3.8024\n",
      "[8250] loss:7.7665 || sec:6.5072\n",
      "[8260] loss:7.0833 || sec:6.4815\n",
      "[8270] loss:7.3749 || sec:6.4896\n",
      "[8280] loss:7.8504 || sec:6.2875\n",
      "[8290] loss:7.9058 || sec:6.5731\n",
      "[8300] loss:8.0498 || sec:6.3298\n",
      "[8310] loss:7.2782 || sec:6.6323\n",
      "[8320] loss:7.9783 || sec:6.3592\n",
      "[8330] loss:7.4213 || sec:6.4654\n",
      "[8340] loss:7.5606 || sec:6.3385\n",
      "[8350] loss:7.4420 || sec:6.2241\n",
      "[8360] loss:7.9276 || sec:6.3141\n",
      "[8370] loss:8.1284 || sec:6.4622\n",
      "[8380] loss:8.0275 || sec:6.6581\n",
      "[8390] loss:7.5574 || sec:6.4142\n",
      "[8400] loss:8.2279 || sec:6.6715\n",
      "[8410] loss:7.7059 || sec:6.3754\n",
      "[47] train_loss: 1390.2576 || val_loss: 0.0000 || sec:117.9641\n",
      "Epoch 48/50\n",
      "[train]\n",
      "[8420] loss:7.6866 || sec:4.6475\n",
      "[8430] loss:7.9629 || sec:6.5169\n",
      "[8440] loss:7.6370 || sec:6.4589\n",
      "[8450] loss:7.4940 || sec:6.2954\n",
      "[8460] loss:7.8843 || sec:6.3548\n",
      "[8470] loss:7.8073 || sec:6.3365\n",
      "[8480] loss:7.7970 || sec:6.4631\n",
      "[8490] loss:7.5879 || sec:6.7290\n",
      "[8500] loss:8.5324 || sec:6.3431\n",
      "[8510] loss:7.4977 || sec:6.4311\n",
      "[8520] loss:7.4765 || sec:6.1967\n",
      "[8530] loss:7.8470 || sec:6.3920\n",
      "[8540] loss:7.9724 || sec:6.5880\n",
      "[8550] loss:7.6875 || sec:6.5123\n",
      "[8560] loss:8.0491 || sec:6.4108\n",
      "[8570] loss:7.5082 || sec:6.5095\n",
      "[8580] loss:7.0984 || sec:6.7116\n",
      "[8590] loss:7.4700 || sec:6.4101\n",
      "[48] train_loss: 1388.2784 || val_loss: 0.0000 || sec:118.2710\n",
      "Epoch 49/50\n",
      "[train]\n",
      "[8600] loss:7.5951 || sec:4.9684\n",
      "[8610] loss:7.9959 || sec:6.2353\n",
      "[8620] loss:7.4132 || sec:6.5807\n",
      "[8630] loss:7.6614 || sec:6.5009\n",
      "[8640] loss:7.5992 || sec:6.7513\n",
      "[8650] loss:7.6986 || sec:6.6105\n",
      "[8660] loss:6.8577 || sec:6.4113\n",
      "[8670] loss:7.9715 || sec:6.3484\n",
      "[8680] loss:7.8507 || sec:6.3506\n",
      "[8690] loss:8.0043 || sec:6.4031\n",
      "[8700] loss:7.7299 || sec:6.4654\n",
      "[8710] loss:7.6265 || sec:6.5639\n",
      "[8720] loss:8.0008 || sec:6.3991\n",
      "[8730] loss:7.4523 || sec:6.4527\n",
      "[8740] loss:7.4592 || sec:6.5086\n",
      "[8750] loss:8.2715 || sec:6.6604\n",
      "[8760] loss:7.8382 || sec:6.2969\n",
      "[8770] loss:7.6429 || sec:6.5240\n",
      "[49] train_loss: 1374.9802 || val_loss: 0.0000 || sec:118.2392\n",
      "Epoch 50/50\n",
      "[train]\n",
      "[8780] loss:7.3170 || sec:5.5907\n",
      "[8790] loss:8.3050 || sec:6.6644\n",
      "[8800] loss:8.1521 || sec:6.3693\n",
      "[8810] loss:7.0091 || sec:6.4638\n",
      "[8820] loss:7.6474 || sec:6.5522\n",
      "[8830] loss:7.7166 || sec:6.4825\n",
      "[8840] loss:8.8964 || sec:6.4474\n",
      "[8850] loss:7.4431 || sec:6.1663\n",
      "[8860] loss:8.2032 || sec:6.5627\n",
      "[8870] loss:8.0649 || sec:6.6960\n",
      "[8880] loss:7.9553 || sec:6.2387\n",
      "[8890] loss:7.8390 || sec:6.5943\n",
      "[8900] loss:8.1152 || sec:6.2261\n",
      "[8910] loss:8.0413 || sec:6.4902\n",
      "[8920] loss:9.7205 || sec:6.3679\n",
      "[8930] loss:8.9343 || sec:6.2732\n",
      "[8940] loss:7.9529 || sec:6.5432\n",
      "[8950] loss:7.9752 || sec:6.2590\n",
      "[val]\n",
      "[50] train_loss: 1443.6367 || val_loss: 1430.3895 || sec:166.6806\n",
      "Epoch 51/50\n",
      "[train]\n",
      "[8960] loss:8.2574 || sec:6.2697\n",
      "[8970] loss:7.7563 || sec:6.1943\n",
      "[8980] loss:7.8076 || sec:6.3359\n",
      "[8990] loss:7.6328 || sec:6.3387\n",
      "[9000] loss:8.3311 || sec:6.3105\n",
      "[9010] loss:8.3456 || sec:6.5216\n",
      "[9020] loss:7.8356 || sec:6.4569\n",
      "[9030] loss:7.6801 || sec:6.1650\n",
      "[9040] loss:8.4342 || sec:6.4661\n",
      "[9050] loss:8.0350 || sec:6.3407\n",
      "[9060] loss:7.7592 || sec:6.3342\n",
      "[9070] loss:7.4322 || sec:6.5662\n",
      "[9080] loss:7.3764 || sec:6.3855\n",
      "[9090] loss:7.9133 || sec:6.6437\n",
      "[9100] loss:8.0484 || sec:6.4283\n",
      "[9110] loss:7.4688 || sec:6.8491\n",
      "[9120] loss:7.1774 || sec:6.7663\n",
      "[51] train_loss: 1410.1954 || val_loss: 0.0000 || sec:117.7458\n"
     ]
    }
   ],
   "source": [
    "num_epochs=50\n",
    "train_model(net,dataloaders_dict,criterion,optimizer,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "415c659548b03e5a7ab126dcfbb4fc7162127f64c5d6abf067896d088a8939b1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorchDeepLearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
